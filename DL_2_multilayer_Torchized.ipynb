{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZgbDF3u_OHl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Многослойный перцептрон\n",
    "\n",
    "В этом блокноте мы реализуем возможность построения полносвязной многослойной нейронной сети при помощи `numpy`. Сначала загрузим требующиеся библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T07:38:15.887840Z",
     "start_time": "2024-10-28T07:38:12.776125Z"
    },
    "id": "05WLHrGG_Jxu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrF6iqhS5HEy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Немного про то, как действуют слои"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvc4016BnaLC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "На данный момент мы намеренно не будем расширять матрицу входных данных значениями $-1$, как мы это делали в однослойной сети, и опишем действие слоя нейронной сети несколько иначе. \n",
    "\n",
    "Пусть нам дан на вход некоторый набор данных $X$ размера $[n \\times p]$, состоящий из $n$ объектов, каждый из которых характеризуется $p$ фичами. Действие любого скрытого слоя нейронной сети можно разбить на два этапа. Первый этап — это действие сумматора, производится оно следующим образом:\n",
    "$$\n",
    "Y_1 = X\\cdot W_0 - b_0,\n",
    "$$\n",
    "где $W_0$ — матрица размера $[p \\times n_1]$, где $n_1$ — количество нейронов следующего слоя, $b_0$ — вектор смещений каждого нейрона (по сути — матрица размера $[n \\times n_1]$, чьи элементы, находящиеся в одном столбце, одинаковы). Итого, на выходе мы получаем для каждого объекта $n_1$ новый признак. Но это не все, второй этап — применение некоторой функции активации $\\varphi_0$ поэлементно ко всем элементам матрицы $Y_1$, полученной этапом ранее:\n",
    "$$\n",
    "Z_1 = \\varphi_0(Y_1).\n",
    "$$\n",
    "Полученная матрица и является входной матрицей для следующего слоя нейронов. \n",
    "\n",
    "Таким образом, имея $(k - 1)$ построенный слой, мы можем построить $k$-ый слой при помощи двух операций:\n",
    "$$\n",
    "Y_k = Z_{k - 1}W_{k-1} - b_{k-1}, \\quad Z_k = \\varphi_{k-1}(Y_k).\n",
    "$$\n",
    "Последний слой обычно выделяют особо, так как этот слой — выходной, и в зависимости от решаемой задачи его выход может разниться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlQolHDp5TIk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Выходной слой при классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Dw2Rgt25ve5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы, опять-таки, будем решать задачу классификации, так что введем еще раз уже известные обозначения.\n",
    "\n",
    "Пусть\n",
    "$$\n",
    "x_i = (x_i^1, ..., x_i^{p}), \\quad i \\in \\{1, 2, ..., n\\},\n",
    "$$\n",
    "— $i$-ый тренировочный объект, $y_i$ — числовая метка класса $i$-ого объекта,\n",
    "$$\n",
    "w^j = (w_1^j, ..., w_{p}^j)^T, \\quad j \\in \\{1, 2, ..., m\\},\n",
    "$$\n",
    "— веса $j$-ого нейрона — столбцы матрицы $W$, $b^j$ — смещение $j$-ого нейрона — столбцы матрицы $b$.\n",
    "\n",
    "Напишем интересующую нас функцию потерь, $y = \\{y_1, ..., y_n\\}$:\n",
    "$$\n",
    "Loss(X, W, y) = -\\frac{1}{n} \\sum\\limits_{i = 1}^n \\ln \\frac{\\exp(x_i \\cdot w^{y_i} - b^{y_i})}{\\sum\\limits_{j = 1}^m\\exp(x_i \\cdot w^j - b^j)} + \\lambda R(w) = -\\frac{1}{n}\\sum\\limits_{i = 1}^n \\left((x_i \\cdot w^{y_i} - b^{y_i}) - \\ln \\sum\\limits_{j = 1}^m\\exp(x_i \\cdot w^j - b^j)\\right) + \\lambda  R(w),\n",
    "$$\n",
    "где \n",
    "$$\n",
    "x_i \\cdot w^j = \\sum\\limits_{k = 1}^px_i^kw^j_k\n",
    "$$\n",
    "— скалярное произведение соответсвующих векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxfchwYU4muJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Для обучения сети нам понадобится градиент этой функции, вычислим его.\n",
    "$$\n",
    "\\frac{\\partial Loss(X, W, y)}{\\partial w^{y'}_p} = -\\frac{1}{n} \\sum\\limits_{i = 1}^n \\left( x_i^p[y_i = y'] - x_i^p \\frac{exp(x_i \\cdot w^{y'})}{\\sum\\limits_{j = 1}^m \\exp(x_i \\cdot w^j)}\\right) + \\lambda  \\frac{\\partial R}{\\partial w}.\n",
    "$$\n",
    "В таком виде градиент использовать неудобно и вычислительно неэффективно. Можно заметить, что в матричном виде он переписывается следующим образом:\n",
    "$$\n",
    "\\frac{\\partial Loss(X, W, y)}{\\partial w} = -\\frac{1}{n}X^T\\left(M - P\\right) + \\lambda \\frac{\\partial R}{\\partial w}.\n",
    "$$\n",
    "Поясним входящие в последнее выражение объекты.  Как мы уже отметили, до применения функции активации выходы нейронов после «сумматора» для набора данных $X$ могут быть получены следущим образом:\n",
    "$$\n",
    "Outs = X \\cdot W - b.\n",
    "$$\n",
    "Матрица $Outs$ имеет размер $[n \\times m]$ и построчно содержит значения выходов каждого из $m$ нейронов для соответсвующего объекта подаваемых данных. Тогда матрица $P$ — это матрица `softmax`-ов для каждого нейрона, на пересечении $i$-ой строки и $t$-ого столбца которой стоят значения\n",
    "$$\n",
    "\\frac{\\exp(x_i \\cdot w^t)}{\\sum\\limits_{j = 1}^m \\exp(x_i \\cdot w^j)}, \\quad i \\in \\{1, 2, ..., n\\}, \\quad t \\in \\{1, 2, ..., m\\},\n",
    "$$\n",
    "$M$ — матрица размера $[n \\times m]$ — разреженная матрица `one_hot` кодированных откликов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG6-e4X_5Zya",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Снова про регуляризацию\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAO4FCvt5zz-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Теперь про регуляризацию. В случае $l_p$, $p \\in \\{1, 2\\}$, регуляризатор имеет вид:\n",
    "$$\n",
    "R_p(W) = \\sum\\limits_{i = 1}^p\\sum\\limits_{j = 1}^m |w_i^j|^p,\n",
    "$$\n",
    "поэтому в матричном виде производная (или градиент) может быть записана так:\n",
    "$$\n",
    "\\frac{\\partial R_2}{\\partial w} = 2\\lambda W, \\quad \\frac{\\partial R_1}{\\partial \\omega} = \\lambda \\operatorname{sign}W.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg6sR3Lp52m8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Теперь о матричном дифференцировании"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FvErTc-6BTf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Матричные операции очень неплохо реализованы в питоне, поэтому именно им и стоит отдать предпочтение при реализации шага градиентного спуска для поиска параметров слоев. Мы уже поняли, что если выходы последнего слоя завязаны на функции потерь, описанной выше, то \n",
    "$$\n",
    "\\frac{\\partial Loss(X, W, y)}{\\partial w} = -\\frac{1}{n}X^T\\left(M - P\\right) + \\lambda \\frac{\\partial R}{\\partial w}.\n",
    "$$\n",
    "Это реализовано в `__get_grad`. В общем случае,\n",
    "$$\n",
    "Z = \\varphi(XW - b), \\quad \\frac{\\partial Z}{\\partial W} = X^T\\frac{\\partial(\\varphi(XW - b))}{\\partial(XW - b)}, \\quad \\frac{\\partial Z}{db} = - \\frac{\\partial(\\varphi(XW - b))}{\\partial(XW - b)}.\n",
    "$$\n",
    "Немного деликатнее оказывается вопрос вычисления более «глубоких» производных, ведь параметры предыдущих слоев находятся внутри $X$, а $X$ получается в результате применения функции активации предыдущего слоя. Поятно, что если\n",
    "$$\n",
    "X = \\psi(\\widetilde X \\widetilde W - \\widetilde b),\n",
    "$$\n",
    "то\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial \\widetilde W} = \\widetilde X^T\\left(\\left(\\frac{\\partial(\\varphi(XW - b))}{\\partial(XW - b)} W^T\\right) \\odot \\frac{\\partial \\psi(X \\widetilde W - \\widetilde b)}{\\partial (X \\widetilde W - \\widetilde b)}\\right),\n",
    "$$\n",
    "и так далее. Это правило цепочки реализовано в самом конце метода `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTN3U0Zv_PIw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Программная реализация и тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T07:38:18.953515Z",
     "start_time": "2024-10-28T07:38:18.939877Z"
    },
    "id": "3FeK3m9ct-_J",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork:\n",
    "    \n",
    "    __REGULARIZATION_GRAD = {None: lambda _w: 0, \"l1\": lambda _w: torch.sign(_w), \"l2\": lambda _w: 2*_w}\n",
    "    __REGULARIZATION_FUNC = {None: lambda _w: 0, \"l1\": lambda _w: torch.abs(_w), \"l2\": lambda _w: _w ** 2}\n",
    "    __LOSS = 0\n",
    "    \n",
    "    # создание нейронной сети: alpha — скорость обучения (шаг градиентного спуска), reg_type — тип регуляризации (если есть), lambda — параметр регуляризации; слои будут храниться в списке layers\n",
    "    def __init__(self, alpha=0.01, reg_type=None, lambda_=0, shpilevoi_gain=0.75):\n",
    "        self.__layers = list()\n",
    "        self.__alpha = alpha\n",
    "        self.__reg_type = reg_type\n",
    "        self.__lambda = lambda_\n",
    "\n",
    "    # метод, позволяющий добавить новый слой: указываем правильные размеры слоя, название функции активации, class_number — количество классов в случае использования Sotmax'а на последнем слое, параметр a — параметр LeakyReLU\n",
    "    def add_layer(self, size: tuple, activation_func: str, class_number=0, a=0):\n",
    "        if not self.__layers or self.__layers[-1].size[1] == size[0]:\n",
    "            self.__layers.append(FullyConnectedLayer(size, activation_func, class_number, a))\n",
    "        else:\n",
    "            raise Exception(\"Wrong size of the layer!\")\n",
    "\n",
    "    def change_alpha(self, alpha):\n",
    "        self.__alpha = alpha\n",
    "\n",
    "    def get_loss(self):\n",
    "        return FullyConnectedNetwork.__LOSS\n",
    "\n",
    "    def get_val_loss(self, data, answers):\n",
    "        layer_outputs = [data]\n",
    "        current_output = layer_outputs[0]\n",
    "        # forward pass и вычисление градиентов функций активации\n",
    "        for layer in self.__layers:\n",
    "            current_output, _ = layer.forward(current_output, answers)\n",
    "\n",
    "        return current_output\n",
    "\n",
    "    # метод, выдающий предсказания для заданного набора данных после обучения модели\n",
    "    def predict(self, data):\n",
    "        current_output = data\n",
    "        for layer in self.__layers[:-1]:\n",
    "            current_output, _ = layer.forward(current_output, None)\n",
    "        # отдельно обрабатываем последний слой\n",
    "        layer_weights, layer_biases = self.__layers[-1].get_weights()\n",
    "        current_output = current_output @ layer_weights - layer_biases\n",
    "        _exp_outputs = (current_output - torch.amax(current_output)).exp()\n",
    "        answ = _exp_outputs / _exp_outputs.sum(axis=1, keepdim=True)\n",
    "        return torch.tensor([torch.argmax(_row) for _row in answ])\n",
    "\n",
    "    def score(self, data, answers):\n",
    "        return (self.predict(data) == answers).sum() / len(answers)\n",
    "\n",
    "    def fit(self, data, answers, use_old_weight):\n",
    "        # выход входного слоя совпадает с фичами входных данных\n",
    "        layer_outputs = [data]\n",
    "        current_output = layer_outputs[0]\n",
    "        grads = []\n",
    "        # forward pass и вычисление градиентов функций активации\n",
    "        for layer in self.__layers:\n",
    "            current_output, gradient = layer.forward(current_output, answers)\n",
    "            layer_outputs.append(current_output)\n",
    "            grads.append(gradient)\n",
    "        # для вычисления градиентов по правилу цепочки, удобно развернуть массив\n",
    "        grads = grads[::-1]\n",
    "        # для градиента параметров самого первого слоя, умножаем на «производную» независимой переменной\n",
    "        grads.append(1)\n",
    "                \n",
    "        current_gradient = grads[0]\n",
    "        FullyConnectedNetwork.__LOSS = layer_outputs[-1]\n",
    "        for i, layer in enumerate(self.__layers[::-1]):\n",
    "            layer_weights, layer_biases = layer.get_weights()\n",
    "            # FullyConnectedNetwork.__LOSS += self.__lambda * (np.sum(FullyConnectedNetwork.__REGULARIZATION_FUNC[self.__reg_type](layer_weights) + FullyConnectedNetwork.__REGULARIZATION_FUNC[self.__reg_type](layer_biases)))\n",
    "            # вычисление градиента параметров W слоя layer\n",
    "            d_weights = layer_outputs[-2 - i].T @ current_gradient\n",
    "            # вычисление градиента параметров db слоя layer\n",
    "            d_bias = (torch.ones(layer_outputs[-2 - i].shape[0]) @ current_gradient) / layer_outputs[-2 - i].shape[0]\n",
    "            # выполнение шага градиентного спуска\n",
    "            \n",
    "            old_w = layer_weights.clone()\n",
    "            \n",
    "            layer.update_weights(self.__alpha * (d_weights + self.__lambda * FullyConnectedNetwork.__REGULARIZATION_GRAD[self.__reg_type](layer_weights)) , self.__alpha * (d_bias + self.__lambda * FullyConnectedNetwork.__REGULARIZATION_GRAD[self.__reg_type](layer_biases)))\n",
    "\n",
    "            if use_old_weight:\n",
    "                current_gradient = np.matmul(current_gradient, old_w.T) * grads[i + 1]\n",
    "            else:\n",
    "                current_gradient = np.matmul(current_gradient, layer_weights.T) * grads[i + 1]\n",
    "                \n",
    "    # метод для отрисовки весов, первый параметр должен быть квадратом целого числа\n",
    "    def print_weights(self, label=\"No label\"):\n",
    "        for layer in self.__layers:\n",
    "            _pixel_weights, _bias = layer.get_weights()\n",
    "            size = tuple([int(round((_pixel_weights.shape[0] - 1) ** 0.5)) for _ in range(2)])\n",
    "            fig, axes = plt.subplots(ncols=5, nrows=2)\n",
    "            fig.set_size_inches(20, 10)\n",
    "            for j in range(10):\n",
    "                axes[j // 5, j % 5].imshow((_pixel_weights[:, j]).reshape(size), cmap=plt.cm.gray, label=label)\n",
    "\n",
    "# класс, отвечающий за слой в нейронной сети\n",
    "class FullyConnectedLayer:\n",
    "    # мы предполагаем, что реализованы следующие функции активации, на последнем слое возможно решение задачи классификации с Softmax\n",
    "    __ACTIVATION_FUNCTIONS = {'ReLU': {'func': lambda a, x: torch.maximum(x, torch.zeros_like(x)), 'derivative': lambda a, x: torch.where(x >= 0, 1, 0)},\n",
    "                              'LReLU': {'func': lambda a, x: torch.where(x >= 0, x, a*x), 'derivative': lambda a, x: torch.where(x >= 0, 1, a)},\n",
    "                              'None': {'func': lambda a, x: x, 'derivative': lambda a, x: 1},\n",
    "                              'Sigmoid': {'func': lambda a, x: torch.exp(x) / (1 + torch.exp(x)), 'derivative': lambda a, x: torch.exp(x) / (1 + torch.exp(x)) ** 2},\n",
    "                              'Softmax': {}}\n",
    "    # создание нового слоя: задание размеров слоя, случайная (равномерная на [-1/2, 1/2]) инициализация весов, запоминание функции активации, фиксация количества классов в случае решения задачи классификации\n",
    "    def __init__(self, size: tuple, activation_func: str, class_number=0, a=0):\n",
    "        self.size = size\n",
    "        torch.manual_seed(42)\n",
    "        self.__weights = torch.rand((size[0], size[1])) - 0.5\n",
    "        torch.manual_seed(42)\n",
    "        self.__bias = torch.rand((1, size[1])) - 0.5\n",
    "        self.__a = a\n",
    "        if activation_func in FullyConnectedLayer.__ACTIVATION_FUNCTIONS.keys():\n",
    "            self.__activation_func = activation_func\n",
    "        else:\n",
    "            raise Exception(\"No such activation function!\")\n",
    "        if activation_func == 'Softmax':\n",
    "            self.__class_number = class_number\n",
    "\n",
    "    # метод, возвращающий значения весов: веса и смещения\n",
    "    def get_weights(self):\n",
    "        return self.__weights, self.__bias\n",
    "\n",
    "    # метод, модифицирующий веса после градиентного шага\n",
    "    def update_weights(self, d_weights, d_biases):\n",
    "        self.__weights -= d_weights\n",
    "        self.__bias -= d_biases\n",
    "\n",
    "    # метод, возвращающий градиент\n",
    "    def __get_grad(self, data, answers):\n",
    "        if self.__activation_func == 'Softmax':\n",
    "            return - (self.__one_hot(answers) - self.__get_probabilities(data)) / len(answers)\n",
    "        else:\n",
    "            return FullyConnectedLayer.__ACTIVATION_FUNCTIONS[self.__activation_func]['derivative'](self.__a, data)\n",
    "\n",
    "    # one-hot encoding меток класса\n",
    "    def __one_hot(self, answers):\n",
    "        one_hot_answers = torch.zeros((len(answers), self.__class_number))\n",
    "        one_hot_answers[torch.arange(len(answers)), answers.int()] = 1\n",
    "        return one_hot_answers\n",
    "\n",
    "    # метод, возвращающий вероятности после Softmax'a\n",
    "    def __get_probabilities(self, data):\n",
    "        _outputs = data @ self.__weights + self.__bias\n",
    "        \n",
    "        _gamma = torch.amax(_outputs)\n",
    "        _exp_outputs = (_outputs - _gamma).exp()\n",
    "        \n",
    "        _exp_outputs = (_outputs).exp()\n",
    "        return (_exp_outputs.T / _exp_outputs.sum(axis=1)).T\n",
    "\n",
    "    # проход по слою с вычислением градиента функции активации на текущей итерации и текущем наборе данных, для последнего слоя нет нужды вычислять значение, если только не хочется узнать что-то про функцию потерь\n",
    "    def forward(self, data, answers):\n",
    "        \n",
    "        if self.__activation_func == 'Softmax':\n",
    "            matrix_pass = data @ self.__weights + self.__bias\n",
    "            print('classical output', matrix_pass.mean().item())\n",
    "            \n",
    "            #my_ce = -((matrix_pass * self.__one_hot(answers)).sum(axis=1, keepdim=True) - matrix_pass.exp().sum(axis=1, keepdim=True).log()).mean()\n",
    "            ce = torch.nn.CrossEntropyLoss()\n",
    "            activation = ce(matrix_pass, answers.long())\n",
    "            gradient = self.__get_grad(data, answers)\n",
    "            \n",
    "        else:\n",
    "            matrix_pass = data @ self.__weights - self.__bias\n",
    "            activation = FullyConnectedLayer.__ACTIVATION_FUNCTIONS[self.__activation_func]['func'](self.__a, matrix_pass)\n",
    "            gradient = self.__get_grad(matrix_pass, answers)\n",
    "        \n",
    "        return activation, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T07:38:22.113666Z",
     "start_time": "2024-10-28T07:38:20.070326Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"archive/fashion-mnist_train.csv\")\n",
    "\n",
    "X = torch.from_numpy(data[data.columns[1:]].values).float()\n",
    "Y = torch.from_numpy(data[data.columns[0]].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T07:38:22.564691Z",
     "start_time": "2024-10-28T07:38:22.117830Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"archive/fashion-mnist_test.csv\")\n",
    "\n",
    "test_x = torch.from_numpy(test_data[test_data.columns[1:]].values).float()\n",
    "test_y = torch.from_numpy(test_data[test_data.columns[0]].values).float()\n",
    "\n",
    "test_x = test_x / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T07:38:22.745852Z",
     "start_time": "2024-10-28T07:38:22.582498Z"
    },
    "id": "XYkvqot7t-_L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train_small = X_train_small / 255\n",
    "X_test_small = X_test_small / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T07:41:21.531079Z",
     "start_time": "2024-10-28T07:41:21.514949Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    \n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield x[i:i + batch_size], y[i:i + batch_size]\n",
    "\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:49:52.161484Z",
     "start_time": "2024-10-28T08:49:52.123037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0\n",
      "---------------------\n",
      "outputs.mean().item()=0.002182572614401579\n",
      "loss.mean()=tensor(2.3081, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0021149194799363613\n",
      "loss.mean()=tensor(2.3059, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0020893639884889126\n",
      "loss.mean()=tensor(2.3061, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0022594064939767122\n",
      "loss.mean()=tensor(2.3063, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0020848079584538937\n",
      "loss.mean()=tensor(2.3030, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0020900170784443617\n",
      "loss.mean()=tensor(2.3010, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0019639625679701567\n",
      "loss.mean()=tensor(2.2991, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0020908694714307785\n",
      "loss.mean()=tensor(2.3015, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.001938970759510994\n",
      "loss.mean()=tensor(2.3030, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0018932728562504053\n",
      "loss.mean()=tensor(2.3025, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.001802500686608255\n",
      "loss.mean()=tensor(2.3003, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0017024403205141425\n",
      "loss.mean()=tensor(2.3048, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.304757595062256\n",
      "ep 1\n",
      "---------------------\n",
      "outputs.mean().item()=0.0014499371172860265\n",
      "loss.mean()=tensor(2.3042, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.001385430688969791\n",
      "loss.mean()=tensor(2.3022, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0013548174174502492\n",
      "loss.mean()=tensor(2.3020, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0015541682951152325\n",
      "loss.mean()=tensor(2.3024, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0013780546141788363\n",
      "loss.mean()=tensor(2.2992, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.001425539143383503\n",
      "loss.mean()=tensor(2.2972, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0012873507803305984\n",
      "loss.mean()=tensor(2.2955, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0014394798781722784\n",
      "loss.mean()=tensor(2.2978, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0012844580924138427\n",
      "loss.mean()=tensor(2.2989, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.001237919321283698\n",
      "loss.mean()=tensor(2.2986, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0011829383438453078\n",
      "loss.mean()=tensor(2.2967, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0010783440666273236\n",
      "loss.mean()=tensor(2.3008, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.300827741622925\n",
      "ep 2\n",
      "---------------------\n",
      "outputs.mean().item()=0.0008327491814270616\n",
      "loss.mean()=tensor(2.3003, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007783883484080434\n",
      "loss.mean()=tensor(2.2985, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007429027464240789\n",
      "loss.mean()=tensor(2.2980, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0009648937848396599\n",
      "loss.mean()=tensor(2.2985, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007946948753669858\n",
      "loss.mean()=tensor(2.2955, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.000886270310729742\n",
      "loss.mean()=tensor(2.2934, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007433289429172873\n",
      "loss.mean()=tensor(2.2918, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0009208963019773364\n",
      "loss.mean()=tensor(2.2941, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007702601724304259\n",
      "loss.mean()=tensor(2.2949, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007337698480114341\n",
      "loss.mean()=tensor(2.2948, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007068639970384538\n",
      "loss.mean()=tensor(2.2932, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0006053486722521484\n",
      "loss.mean()=tensor(2.2970, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2970123291015625\n",
      "ep 3\n",
      "---------------------\n",
      "outputs.mean().item()=0.00036306059337221086\n",
      "loss.mean()=tensor(2.2965, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00031479797326028347\n",
      "loss.mean()=tensor(2.2950, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00027884510927833617\n",
      "loss.mean()=tensor(2.2940, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0005268178065307438\n",
      "loss.mean()=tensor(2.2948, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0003696329949889332\n",
      "loss.mean()=tensor(2.2920, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0004929577116854489\n",
      "loss.mean()=tensor(2.2898, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0003371704078745097\n",
      "loss.mean()=tensor(2.2883, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0005443174159154296\n",
      "loss.mean()=tensor(2.2905, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00040309218456968665\n",
      "loss.mean()=tensor(2.2911, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0003824513405561447\n",
      "loss.mean()=tensor(2.2912, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0003703149559441954\n",
      "loss.mean()=tensor(2.2898, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0002778093912638724\n",
      "loss.mean()=tensor(2.2933, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.29331636428833\n",
      "ep 4\n",
      "---------------------\n",
      "outputs.mean().item()=4.9127313104690984e-05\n",
      "loss.mean()=tensor(2.2929, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=2.3327768303715857e-06\n",
      "loss.mean()=tensor(2.2916, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-3.921855386579409e-05\n",
      "loss.mean()=tensor(2.2902, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00022553997405339032\n",
      "loss.mean()=tensor(2.2912, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=7.753698446322232e-05\n",
      "loss.mean()=tensor(2.2885, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0002368144632782787\n",
      "loss.mean()=tensor(2.2862, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=5.860054079676047e-05\n",
      "loss.mean()=tensor(2.2849, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0002928721660282463\n",
      "loss.mean()=tensor(2.2870, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00016061076894402504\n",
      "loss.mean()=tensor(2.2874, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00015369341417681426\n",
      "loss.mean()=tensor(2.2876, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00015956853167153895\n",
      "loss.mean()=tensor(2.2865, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=6.972441042307764e-05\n",
      "loss.mean()=tensor(2.2897, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.28969144821167\n",
      "ep 5\n",
      "---------------------\n",
      "outputs.mean().item()=-0.00012774574861396104\n",
      "loss.mean()=tensor(2.2893, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-0.00017549781478010118\n",
      "loss.mean()=tensor(2.2881, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-0.00021323170221876353\n",
      "loss.mean()=tensor(2.2865, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=6.491188105428591e-05\n",
      "loss.mean()=tensor(2.2877, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-7.115704647731036e-05\n",
      "loss.mean()=tensor(2.2851, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00012405970483087003\n",
      "loss.mean()=tensor(2.2827, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-7.104594260454178e-05\n",
      "loss.mean()=tensor(2.2815, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00019678976968862116\n",
      "loss.mean()=tensor(2.2836, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=8.077323582256213e-05\n",
      "loss.mean()=tensor(2.2837, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=8.399812213610858e-05\n",
      "loss.mean()=tensor(2.2841, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00011169658682774752\n",
      "loss.mean()=tensor(2.2832, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=2.6823256121133454e-05\n",
      "loss.mean()=tensor(2.2861, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2860617637634277\n",
      "ep 6\n",
      "---------------------\n",
      "outputs.mean().item()=-0.000135989481350407\n",
      "loss.mean()=tensor(2.2858, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-0.00017947665764950216\n",
      "loss.mean()=tensor(2.2847, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-0.00020922385738231242\n",
      "loss.mean()=tensor(2.2828, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=9.640166535973549e-05\n",
      "loss.mean()=tensor(2.2842, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-1.9752140360651538e-05\n",
      "loss.mean()=tensor(2.2817, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00020600789866875857\n",
      "loss.mean()=tensor(2.2792, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-1.4278106164056226e-06\n",
      "loss.mean()=tensor(2.2781, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00029868539422750473\n",
      "loss.mean()=tensor(2.2800, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0002050948387477547\n",
      "loss.mean()=tensor(2.2800, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0002115238457918167\n",
      "loss.mean()=tensor(2.2805, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0002577795530669391\n",
      "loss.mean()=tensor(2.2797, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00017904677952174097\n",
      "loss.mean()=tensor(2.2824, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2823619842529297\n",
      "ep 7\n",
      "---------------------\n",
      "outputs.mean().item()=5.2210594731150195e-05\n",
      "loss.mean()=tensor(2.2821, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=9.60174929787172e-06\n",
      "loss.mean()=tensor(2.2812, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=-7.657892638235353e-06\n",
      "loss.mean()=tensor(2.2790, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00032563702552579343\n",
      "loss.mean()=tensor(2.2805, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00021912176453042775\n",
      "loss.mean()=tensor(2.2783, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00047487905248999596\n",
      "loss.mean()=tensor(2.2755, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00024842694983817637\n",
      "loss.mean()=tensor(2.2745, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0005745504167862236\n",
      "loss.mean()=tensor(2.2763, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.000508119526784867\n",
      "loss.mean()=tensor(2.2762, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0005122871370986104\n",
      "loss.mean()=tensor(2.2767, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0005718149477615952\n",
      "loss.mean()=tensor(2.2762, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0004929713322781026\n",
      "loss.mean()=tensor(2.2785, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.278524875640869\n",
      "ep 8\n",
      "---------------------\n",
      "outputs.mean().item()=0.0004038483020849526\n",
      "loss.mean()=tensor(2.2783, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0003431093064136803\n",
      "loss.mean()=tensor(2.2775, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0003366571036167443\n",
      "loss.mean()=tensor(2.2750, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007033332949504256\n",
      "loss.mean()=tensor(2.2767, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0006054163095541298\n",
      "loss.mean()=tensor(2.2746, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0008871309692040086\n",
      "loss.mean()=tensor(2.2716, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0006415763637050986\n",
      "loss.mean()=tensor(2.2709, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0009939547162503004\n",
      "loss.mean()=tensor(2.2725, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0009490713709965348\n",
      "loss.mean()=tensor(2.2722, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0009418124100193381\n",
      "loss.mean()=tensor(2.2728, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0010098603088408709\n",
      "loss.mean()=tensor(2.2725, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.000931120419409126\n",
      "loss.mean()=tensor(2.2745, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2745134830474854\n",
      "ep 9\n",
      "---------------------\n",
      "outputs.mean().item()=0.0008728777756914496\n",
      "loss.mean()=tensor(2.2744, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0007852921262383461\n",
      "loss.mean()=tensor(2.2737, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.000795507978182286\n",
      "loss.mean()=tensor(2.2709, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0011921022087335587\n",
      "loss.mean()=tensor(2.2728, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0011007763678207994\n",
      "loss.mean()=tensor(2.2709, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0014010932063683867\n",
      "loss.mean()=tensor(2.2676, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.001128142117522657\n",
      "loss.mean()=tensor(2.2670, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0015012102667242289\n",
      "loss.mean()=tensor(2.2685, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0014816569164395332\n",
      "loss.mean()=tensor(2.2680, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0014574856031686068\n",
      "loss.mean()=tensor(2.2687, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0015261046355590224\n",
      "loss.mean()=tensor(2.2686, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0014425059780478477\n",
      "loss.mean()=tensor(2.2703, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.270310878753662\n",
      "ep 10\n",
      "---------------------\n",
      "outputs.mean().item()=0.001413449994288385\n",
      "loss.mean()=tensor(2.2703, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.001294504152610898\n",
      "loss.mean()=tensor(2.2696, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0013268528273329139\n",
      "loss.mean()=tensor(2.2666, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0017466966528445482\n",
      "loss.mean()=tensor(2.2686, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0016585172852501273\n",
      "loss.mean()=tensor(2.2669, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0019749419298022985\n",
      "loss.mean()=tensor(2.2634, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0016784949693828821\n",
      "loss.mean()=tensor(2.2630, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0020692001562565565\n",
      "loss.mean()=tensor(2.2643, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.002076053526252508\n",
      "loss.mean()=tensor(2.2636, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.002031164476647973\n",
      "loss.mean()=tensor(2.2644, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0020965547300875187\n",
      "loss.mean()=tensor(2.2645, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.002008294453844428\n",
      "loss.mean()=tensor(2.2659, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.265899419784546\n",
      "ep 11\n",
      "---------------------\n",
      "outputs.mean().item()=0.002006382681429386\n",
      "loss.mean()=tensor(2.2659, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0018541679019108415\n",
      "loss.mean()=tensor(2.2654, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0019040219485759735\n",
      "loss.mean()=tensor(2.2620, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0023532141931355\n",
      "loss.mean()=tensor(2.2642, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.002274382160976529\n",
      "loss.mean()=tensor(2.2627, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0026001781225204468\n",
      "loss.mean()=tensor(2.2589, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0022737435065209866\n",
      "loss.mean()=tensor(2.2587, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.002680998295545578\n",
      "loss.mean()=tensor(2.2598, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0027184076607227325\n",
      "loss.mean()=tensor(2.2589, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0026510378811508417\n",
      "loss.mean()=tensor(2.2598, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0027190709952265024\n",
      "loss.mean()=tensor(2.2601, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.002622685395181179\n",
      "loss.mean()=tensor(2.2612, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2612221240997314\n",
      "ep 12\n",
      "---------------------\n",
      "outputs.mean().item()=0.0026523510459810495\n",
      "loss.mean()=tensor(2.2612, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0024664478842169046\n",
      "loss.mean()=tensor(2.2608, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.002536692889407277\n",
      "loss.mean()=tensor(2.2572, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.003015900729224086\n",
      "loss.mean()=tensor(2.2596, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0029484503902494907\n",
      "loss.mean()=tensor(2.2582, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0032813739962875843\n",
      "loss.mean()=tensor(2.2541, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0029272870160639286\n",
      "loss.mean()=tensor(2.2541, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0033513163216412067\n",
      "loss.mean()=tensor(2.2551, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0034198802895843983\n",
      "loss.mean()=tensor(2.2539, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0033319711219519377\n",
      "loss.mean()=tensor(2.2549, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0034039183519780636\n",
      "loss.mean()=tensor(2.2554, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0032949859742075205\n",
      "loss.mean()=tensor(2.2562, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.256211042404175\n",
      "ep 13\n",
      "---------------------\n",
      "outputs.mean().item()=0.0033532404340803623\n",
      "loss.mean()=tensor(2.2562, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0031377114355564117\n",
      "loss.mean()=tensor(2.2559, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.003224256681278348\n",
      "loss.mean()=tensor(2.2520, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0037342067807912827\n",
      "loss.mean()=tensor(2.2545, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.003671511309221387\n",
      "loss.mean()=tensor(2.2534, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004011283628642559\n",
      "loss.mean()=tensor(2.2489, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.003633736167103052\n",
      "loss.mean()=tensor(2.2492, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004079337231814861\n",
      "loss.mean()=tensor(2.2499, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0041810935363173485\n",
      "loss.mean()=tensor(2.2486, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0040674954652786255\n",
      "loss.mean()=tensor(2.2496, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0041401139460504055\n",
      "loss.mean()=tensor(2.2503, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0040205735713243484\n",
      "loss.mean()=tensor(2.2508, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2508177757263184\n",
      "ep 14\n",
      "---------------------\n",
      "outputs.mean().item()=0.004112238995730877\n",
      "loss.mean()=tensor(2.2508, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0038638939149677753\n",
      "loss.mean()=tensor(2.2506, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.003966368734836578\n",
      "loss.mean()=tensor(2.2464, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004507963079959154\n",
      "loss.mean()=tensor(2.2491, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00444915983825922\n",
      "loss.mean()=tensor(2.2482, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004795219283550978\n",
      "loss.mean()=tensor(2.2433, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0043916222639381886\n",
      "loss.mean()=tensor(2.2438, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004853495396673679\n",
      "loss.mean()=tensor(2.2444, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004990922287106514\n",
      "loss.mean()=tensor(2.2428, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004849535878747702\n",
      "loss.mean()=tensor(2.2439, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0049251532182097435\n",
      "loss.mean()=tensor(2.2448, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004789245780557394\n",
      "loss.mean()=tensor(2.2450, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.244988203048706\n",
      "ep 15\n",
      "---------------------\n",
      "outputs.mean().item()=0.004912571050226688\n",
      "loss.mean()=tensor(2.2450, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004630191717296839\n",
      "loss.mean()=tensor(2.2449, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.004751028027385473\n",
      "loss.mean()=tensor(2.2404, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005322829820215702\n",
      "loss.mean()=tensor(2.2432, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005270198918879032\n",
      "loss.mean()=tensor(2.2425, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005620909389108419\n",
      "loss.mean()=tensor(2.2373, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005195409990847111\n",
      "loss.mean()=tensor(2.2380, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005675558000802994\n",
      "loss.mean()=tensor(2.2384, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00585144991055131\n",
      "loss.mean()=tensor(2.2365, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005677917972207069\n",
      "loss.mean()=tensor(2.2378, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005752291064709425\n",
      "loss.mean()=tensor(2.2389, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005607586354017258\n",
      "loss.mean()=tensor(2.2387, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.238666296005249\n",
      "ep 16\n",
      "---------------------\n",
      "outputs.mean().item()=0.005765148904174566\n",
      "loss.mean()=tensor(2.2386, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005445036571472883\n",
      "loss.mean()=tensor(2.2387, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.005584541708230972\n",
      "loss.mean()=tensor(2.2338, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006188641767948866\n",
      "loss.mean()=tensor(2.2369, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006150644272565842\n",
      "loss.mean()=tensor(2.2363, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006497442722320557\n",
      "loss.mean()=tensor(2.2307, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006048082374036312\n",
      "loss.mean()=tensor(2.2317, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0065462649799883366\n",
      "loss.mean()=tensor(2.2319, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006758467759937048\n",
      "loss.mean()=tensor(2.2297, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0065506198443472385\n",
      "loss.mean()=tensor(2.2311, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006624695844948292\n",
      "loss.mean()=tensor(2.2324, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006472927052527666\n",
      "loss.mean()=tensor(2.2318, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2317898273468018\n",
      "ep 17\n",
      "---------------------\n",
      "outputs.mean().item()=0.006656946148723364\n",
      "loss.mean()=tensor(2.2317, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006296875420957804\n",
      "loss.mean()=tensor(2.2319, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006456981413066387\n",
      "loss.mean()=tensor(2.2267, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0070975408889353275\n",
      "loss.mean()=tensor(2.2299, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0070739141665399075\n",
      "loss.mean()=tensor(2.2295, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.007413418497890234\n",
      "loss.mean()=tensor(2.2236, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.006938302423804998\n",
      "loss.mean()=tensor(2.2248, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0074543715454638\n",
      "loss.mean()=tensor(2.2248, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0077070617116987705\n",
      "loss.mean()=tensor(2.2222, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.007461732719093561\n",
      "loss.mean()=tensor(2.2238, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.007541020400822163\n",
      "loss.mean()=tensor(2.2254, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.007380941417068243\n",
      "loss.mean()=tensor(2.2243, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.2242753505706787\n",
      "ep 18\n",
      "---------------------\n",
      "outputs.mean().item()=0.0076012639328837395\n",
      "loss.mean()=tensor(2.2242, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.007190989796072245\n",
      "loss.mean()=tensor(2.2245, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.007373235188424587\n",
      "loss.mean()=tensor(2.2189, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008055957034230232\n",
      "loss.mean()=tensor(2.2223, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00804512295871973\n",
      "loss.mean()=tensor(2.2221, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008379521779716015\n",
      "loss.mean()=tensor(2.2157, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.007880233228206635\n",
      "loss.mean()=tensor(2.2172, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008414322510361671\n",
      "loss.mean()=tensor(2.2171, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008708338253200054\n",
      "loss.mean()=tensor(2.2141, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008429668843746185\n",
      "loss.mean()=tensor(2.2159, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008503878489136696\n",
      "loss.mean()=tensor(2.2176, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00833473913371563\n",
      "loss.mean()=tensor(2.2161, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.216055154800415\n",
      "ep 19\n",
      "---------------------\n",
      "outputs.mean().item()=0.008591133169829845\n",
      "loss.mean()=tensor(2.2160, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008127895183861256\n",
      "loss.mean()=tensor(2.2164, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00833631120622158\n",
      "loss.mean()=tensor(2.2104, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009065134450793266\n",
      "loss.mean()=tensor(2.2140, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009069656953215599\n",
      "loss.mean()=tensor(2.2140, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009396674111485481\n",
      "loss.mean()=tensor(2.2072, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.008873905055224895\n",
      "loss.mean()=tensor(2.2089, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009430855512619019\n",
      "loss.mean()=tensor(2.2086, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009763156063854694\n",
      "loss.mean()=tensor(2.2052, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009452172555029392\n",
      "loss.mean()=tensor(2.2071, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.00952534656971693\n",
      "loss.mean()=tensor(2.2092, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009345531463623047\n",
      "loss.mean()=tensor(2.2070, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.207036018371582\n",
      "ep 20\n",
      "---------------------\n",
      "outputs.mean().item()=0.009636884555220604\n",
      "loss.mean()=tensor(2.2069, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009122108109295368\n",
      "loss.mean()=tensor(2.2074, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009358266368508339\n",
      "loss.mean()=tensor(2.2010, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.010131827555596828\n",
      "loss.mean()=tensor(2.2048, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01015189103782177\n",
      "loss.mean()=tensor(2.2051, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01047330442816019\n",
      "loss.mean()=tensor(2.1978, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.009920863434672356\n",
      "loss.mean()=tensor(2.1998, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.010505050420761108\n",
      "loss.mean()=tensor(2.1993, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.010878833942115307\n",
      "loss.mean()=tensor(2.1954, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.010533448308706284\n",
      "loss.mean()=tensor(2.1976, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01061017531901598\n",
      "loss.mean()=tensor(2.1999, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.010417478159070015\n",
      "loss.mean()=tensor(2.1971, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.1971397399902344\n",
      "ep 21\n",
      "---------------------\n",
      "outputs.mean().item()=0.010746195912361145\n",
      "loss.mean()=tensor(2.1969, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.010173733346164227\n",
      "loss.mean()=tensor(2.1976, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.010441888123750687\n",
      "loss.mean()=tensor(2.1908, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011264427565038204\n",
      "loss.mean()=tensor(2.1947, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011302179656922817\n",
      "loss.mean()=tensor(2.1953, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011617482639849186\n",
      "loss.mean()=tensor(2.1874, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011029061861336231\n",
      "loss.mean()=tensor(2.1898, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011649747379124165\n",
      "loss.mean()=tensor(2.1890, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012059604749083519\n",
      "loss.mean()=tensor(2.1846, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011678568087518215\n",
      "loss.mean()=tensor(2.1871, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011757193133234978\n",
      "loss.mean()=tensor(2.1896, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0115512041375041\n",
      "loss.mean()=tensor(2.1862, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.1862499713897705\n",
      "ep 22\n",
      "---------------------\n",
      "outputs.mean().item()=0.011918574571609497\n",
      "loss.mean()=tensor(2.1859, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01129337027668953\n",
      "loss.mean()=tensor(2.1868, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.011597353033721447\n",
      "loss.mean()=tensor(2.1795, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012475754134356976\n",
      "loss.mean()=tensor(2.1836, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012530038133263588\n",
      "loss.mean()=tensor(2.1845, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0128385741263628\n",
      "loss.mean()=tensor(2.1760, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012215953320264816\n",
      "loss.mean()=tensor(2.1787, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01288040541112423\n",
      "loss.mean()=tensor(2.1777, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.013335064053535461\n",
      "loss.mean()=tensor(2.1728, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012919631786644459\n",
      "loss.mean()=tensor(2.1755, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012994147837162018\n",
      "loss.mean()=tensor(2.1783, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012776895426213741\n",
      "loss.mean()=tensor(2.1743, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.174253463745117\n",
      "ep 23\n",
      "---------------------\n",
      "outputs.mean().item()=0.01318342424929142\n",
      "loss.mean()=tensor(2.1738, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012499189004302025\n",
      "loss.mean()=tensor(2.1749, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.012836295180022717\n",
      "loss.mean()=tensor(2.1670, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01378022599965334\n",
      "loss.mean()=tensor(2.1714, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.013853022828698158\n",
      "loss.mean()=tensor(2.1725, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014160471968352795\n",
      "loss.mean()=tensor(2.1634, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01350865326821804\n",
      "loss.mean()=tensor(2.1665, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014214282855391502\n",
      "loss.mean()=tensor(2.1653, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014717337675392628\n",
      "loss.mean()=tensor(2.1597, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014264421537518501\n",
      "loss.mean()=tensor(2.1627, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014333781786262989\n",
      "loss.mean()=tensor(2.1658, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014102539978921413\n",
      "loss.mean()=tensor(2.1610, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.1610360145568848\n",
      "ep 24\n",
      "---------------------\n",
      "outputs.mean().item()=0.01455733459442854\n",
      "loss.mean()=tensor(2.1604, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.013808613643050194\n",
      "loss.mean()=tensor(2.1617, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014182711020112038\n",
      "loss.mean()=tensor(2.1533, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01519615389406681\n",
      "loss.mean()=tensor(2.1580, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.015289762988686562\n",
      "loss.mean()=tensor(2.1593, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.015591728501021862\n",
      "loss.mean()=tensor(2.1495, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.014898890629410744\n",
      "loss.mean()=tensor(2.1531, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.015656504780054092\n",
      "loss.mean()=tensor(2.1516, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.016216382384300232\n",
      "loss.mean()=tensor(2.1453, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.015719007700681686\n",
      "loss.mean()=tensor(2.1487, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.015780720859766006\n",
      "loss.mean()=tensor(2.1520, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01553005538880825\n",
      "loss.mean()=tensor(2.1465, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.1464593410491943\n",
      "ep 25\n",
      "---------------------\n",
      "outputs.mean().item()=0.016030872240662575\n",
      "loss.mean()=tensor(2.1457, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.015211358666419983\n",
      "loss.mean()=tensor(2.1471, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.015629401430487633\n",
      "loss.mean()=tensor(2.1382, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.016716618090867996\n",
      "loss.mean()=tensor(2.1431, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.016829438507556915\n",
      "loss.mean()=tensor(2.1447, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.017133081331849098\n",
      "loss.mean()=tensor(2.1342, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.016392996534705162\n",
      "loss.mean()=tensor(2.1382, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.017208170145750046\n",
      "loss.mean()=tensor(2.1364, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0178281981498003\n",
      "loss.mean()=tensor(2.1294, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0172810610383749\n",
      "loss.mean()=tensor(2.1331, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01733485795557499\n",
      "loss.mean()=tensor(2.1368, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.017062019556760788\n",
      "loss.mean()=tensor(2.1304, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.130368232727051\n",
      "ep 26\n",
      "---------------------\n",
      "outputs.mean().item()=0.017607619985938072\n",
      "loss.mean()=tensor(2.1294, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.016706760972738266\n",
      "loss.mean()=tensor(2.1310, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.017161745578050613\n",
      "loss.mean()=tensor(2.1216, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01833624765276909\n",
      "loss.mean()=tensor(2.1267, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.018467949703335762\n",
      "loss.mean()=tensor(2.1287, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.018771808594465256\n",
      "loss.mean()=tensor(2.1172, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.017978142946958542\n",
      "loss.mean()=tensor(2.1217, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.018855396658182144\n",
      "loss.mean()=tensor(2.1197, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01954597793519497\n",
      "loss.mean()=tensor(2.1119, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.018936702981591225\n",
      "loss.mean()=tensor(2.1159, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.01899072155356407\n",
      "loss.mean()=tensor(2.1199, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.018695412203669548\n",
      "loss.mean()=tensor(2.1126, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.1126177310943604\n",
      "ep 27\n",
      "---------------------\n",
      "outputs.mean().item()=0.019294654950499535\n",
      "loss.mean()=tensor(2.1114, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.018304478377103806\n",
      "loss.mean()=tensor(2.1131, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.018802203238010406\n",
      "loss.mean()=tensor(2.1032, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.020077034831047058\n",
      "loss.mean()=tensor(2.1086, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.020233502611517906\n",
      "loss.mean()=tensor(2.1109, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02053324319422245\n",
      "loss.mean()=tensor(2.0984, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.019689852371811867\n",
      "loss.mean()=tensor(2.1036, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.020632177591323853\n",
      "loss.mean()=tensor(2.1012, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.021394487470388412\n",
      "loss.mean()=tensor(2.0926, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.020717445760965347\n",
      "loss.mean()=tensor(2.0970, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02076086401939392\n",
      "loss.mean()=tensor(2.1013, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.020441623404622078\n",
      "loss.mean()=tensor(2.0931, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.0930685997009277\n",
      "ep 28\n",
      "---------------------\n",
      "outputs.mean().item()=0.02109716273844242\n",
      "loss.mean()=tensor(2.0915, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.020000193268060684\n",
      "loss.mean()=tensor(2.0934, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.020548362284898758\n",
      "loss.mean()=tensor(2.0829, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.021932002156972885\n",
      "loss.mean()=tensor(2.0887, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.022114723920822144\n",
      "loss.mean()=tensor(2.0914, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.022408297285437584\n",
      "loss.mean()=tensor(2.0776, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0215013325214386\n",
      "loss.mean()=tensor(2.0836, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02250514179468155\n",
      "loss.mean()=tensor(2.0808, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.023356497287750244\n",
      "loss.mean()=tensor(2.0714, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.022607820108532906\n",
      "loss.mean()=tensor(2.0761, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.022640449926257133\n",
      "loss.mean()=tensor(2.0808, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.022301284596323967\n",
      "loss.mean()=tensor(2.0716, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.071575164794922\n",
      "ep 29\n",
      "---------------------\n",
      "outputs.mean().item()=0.023003557696938515\n",
      "loss.mean()=tensor(2.0696, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.021775582805275917\n",
      "loss.mean()=tensor(2.0716, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.022384172305464745\n",
      "loss.mean()=tensor(2.0605, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.023890236392617226\n",
      "loss.mean()=tensor(2.0667, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0241030752658844\n",
      "loss.mean()=tensor(2.0699, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.024387314915657043\n",
      "loss.mean()=tensor(2.0548, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02341117151081562\n",
      "loss.mean()=tensor(2.0617, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.024488721042871475\n",
      "loss.mean()=tensor(2.0583, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.025432899594306946\n",
      "loss.mean()=tensor(2.0481, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.024607520550489426\n",
      "loss.mean()=tensor(2.0531, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02461913786828518\n",
      "loss.mean()=tensor(2.0581, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02425769902765751\n",
      "loss.mean()=tensor(2.0480, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.048001766204834\n",
      "ep 30\n",
      "---------------------\n",
      "outputs.mean().item()=0.02500937506556511\n",
      "loss.mean()=tensor(2.0456, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.023641550913453102\n",
      "loss.mean()=tensor(2.0476, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.024313347414135933\n",
      "loss.mean()=tensor(2.0360, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02596038021147251\n",
      "loss.mean()=tensor(2.0426, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02620530128479004\n",
      "loss.mean()=tensor(2.0464, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02647397853434086\n",
      "loss.mean()=tensor(2.0296, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02542155422270298\n",
      "loss.mean()=tensor(2.0376, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0265788733959198\n",
      "loss.mean()=tensor(2.0337, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02761382982134819\n",
      "loss.mean()=tensor(2.0227, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.026690984144806862\n",
      "loss.mean()=tensor(2.0279, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.026686523109674454\n",
      "loss.mean()=tensor(2.0333, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.026282114908099174\n",
      "loss.mean()=tensor(2.0222, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=2.022181272506714\n",
      "ep 31\n",
      "---------------------\n",
      "outputs.mean().item()=0.02710779570043087\n",
      "loss.mean()=tensor(2.0192, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.025586605072021484\n",
      "loss.mean()=tensor(2.0212, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02632891573011875\n",
      "loss.mean()=tensor(2.0091, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.028124088421463966\n",
      "loss.mean()=tensor(2.0162, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02840418554842472\n",
      "loss.mean()=tensor(2.0207, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.028655916452407837\n",
      "loss.mean()=tensor(2.0019, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.027525385841727257\n",
      "loss.mean()=tensor(2.0113, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.028762420639395714\n",
      "loss.mean()=tensor(2.0067, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.029913533478975296\n",
      "loss.mean()=tensor(1.9949, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02887932024896145\n",
      "loss.mean()=tensor(2.0003, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0288497693836689\n",
      "loss.mean()=tensor(2.0060, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.028401726856827736\n",
      "loss.mean()=tensor(1.9940, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.99397611618042\n",
      "ep 32\n",
      "---------------------\n",
      "outputs.mean().item()=0.029286455363035202\n",
      "loss.mean()=tensor(1.9903, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02758183516561985\n",
      "loss.mean()=tensor(1.9923, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.028392309322953224\n",
      "loss.mean()=tensor(1.9798, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03035966120660305\n",
      "loss.mean()=tensor(1.9874, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0306734386831522\n",
      "loss.mean()=tensor(1.9928, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.030895298346877098\n",
      "loss.mean()=tensor(1.9717, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02968699112534523\n",
      "loss.mean()=tensor(1.9825, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.030998727306723595\n",
      "loss.mean()=tensor(1.9772, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03227321058511734\n",
      "loss.mean()=tensor(1.9646, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03112047351896763\n",
      "loss.mean()=tensor(1.9701, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.031070511788129807\n",
      "loss.mean()=tensor(1.9761, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03057284839451313\n",
      "loss.mean()=tensor(1.9633, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.9633179903030396\n",
      "ep 33\n",
      "---------------------\n",
      "outputs.mean().item()=0.0315115824341774\n",
      "loss.mean()=tensor(1.9588, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.02960914932191372\n",
      "loss.mean()=tensor(1.9607, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03048085793852806\n",
      "loss.mean()=tensor(1.9478, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.032642874866724014\n",
      "loss.mean()=tensor(1.9561, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03299097344279289\n",
      "loss.mean()=tensor(1.9624, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.033185239881277084\n",
      "loss.mean()=tensor(1.9388, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03188120946288109\n",
      "loss.mean()=tensor(1.9513, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0332711860537529\n",
      "loss.mean()=tensor(1.9452, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03467793017625809\n",
      "loss.mean()=tensor(1.9319, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03338327258825302\n",
      "loss.mean()=tensor(1.9374, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.033306486904621124\n",
      "loss.mean()=tensor(1.9436, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03274407237768173\n",
      "loss.mean()=tensor(1.9302, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.9301769733428955\n",
      "ep 34\n",
      "---------------------\n",
      "outputs.mean().item()=0.03373272344470024\n",
      "loss.mean()=tensor(1.9246, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03160260245203972\n",
      "loss.mean()=tensor(1.9264, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.032531559467315674\n",
      "loss.mean()=tensor(1.9132, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03491097316145897\n",
      "loss.mean()=tensor(1.9223, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.035315580666065216\n",
      "loss.mean()=tensor(1.9298, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03546161204576492\n",
      "loss.mean()=tensor(1.9032, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.034043677151203156\n",
      "loss.mean()=tensor(1.9176, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03551115468144417\n",
      "loss.mean()=tensor(1.9107, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.037073440849781036\n",
      "loss.mean()=tensor(1.8967, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03562616929411888\n",
      "loss.mean()=tensor(1.9022, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03552796691656113\n",
      "loss.mean()=tensor(1.9085, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03490793704986572\n",
      "loss.mean()=tensor(1.8946, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.8945947885513306\n",
      "ep 35\n",
      "---------------------\n",
      "outputs.mean().item()=0.035967014729976654\n",
      "loss.mean()=tensor(1.8879, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.033580757677555084\n",
      "loss.mean()=tensor(1.8895, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03456179425120354\n",
      "loss.mean()=tensor(1.8760, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03717615082859993\n",
      "loss.mean()=tensor(1.8859, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03763245791196823\n",
      "loss.mean()=tensor(1.8948, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03772759437561035\n",
      "loss.mean()=tensor(1.8650, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03617772459983826\n",
      "loss.mean()=tensor(1.8816, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03774508088827133\n",
      "loss.mean()=tensor(1.8737, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.039464451372623444\n",
      "loss.mean()=tensor(1.8592, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03784770146012306\n",
      "loss.mean()=tensor(1.8645, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.037717513740062714\n",
      "loss.mean()=tensor(1.8708, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03703800216317177\n",
      "loss.mean()=tensor(1.8567, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.8567222356796265\n",
      "ep 36\n",
      "---------------------\n",
      "outputs.mean().item()=0.03815755248069763\n",
      "loss.mean()=tensor(1.8488, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.035506851971149445\n",
      "loss.mean()=tensor(1.8500, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.036549948155879974\n",
      "loss.mean()=tensor(1.8363, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.039444029331207275\n",
      "loss.mean()=tensor(1.8473, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.039972350001335144\n",
      "loss.mean()=tensor(1.8578, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0400141216814518\n",
      "loss.mean()=tensor(1.8244, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.038319092243909836\n",
      "loss.mean()=tensor(1.8433, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03999091312289238\n",
      "loss.mean()=tensor(1.8345, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04189299792051315\n",
      "loss.mean()=tensor(1.8195, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04010424762964249\n",
      "loss.mean()=tensor(1.8245, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.039947520941495895\n",
      "loss.mean()=tensor(1.8308, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.039176248013973236\n",
      "loss.mean()=tensor(1.8169, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.8168683052062988\n",
      "ep 37\n",
      "---------------------\n",
      "outputs.mean().item()=0.040384482592344284\n",
      "loss.mean()=tensor(1.8075, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.037438638508319855\n",
      "loss.mean()=tensor(1.8084, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.03853248432278633\n",
      "loss.mean()=tensor(1.7946, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04174455255270004\n",
      "loss.mean()=tensor(1.8066, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.042342666536569595\n",
      "loss.mean()=tensor(1.8191, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04234510287642479\n",
      "loss.mean()=tensor(1.7818, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.040469806641340256\n",
      "loss.mean()=tensor(1.8033, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0422772578895092\n",
      "loss.mean()=tensor(1.7935, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.044360943138599396\n",
      "loss.mean()=tensor(1.7782, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04240933805704117\n",
      "loss.mean()=tensor(1.7828, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04222044721245766\n",
      "loss.mean()=tensor(1.7889, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04136081039905548\n",
      "loss.mean()=tensor(1.7755, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.7755142450332642\n",
      "ep 38\n",
      "---------------------\n",
      "outputs.mean().item()=0.04266239330172539\n",
      "loss.mean()=tensor(1.7645, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.039402976632118225\n",
      "loss.mean()=tensor(1.7652, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04055939242243767\n",
      "loss.mean()=tensor(1.7513, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04410145431756973\n",
      "loss.mean()=tensor(1.7645, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04478960111737251\n",
      "loss.mean()=tensor(1.7790, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.044745318591594696\n",
      "loss.mean()=tensor(1.7378, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.042704980820417404\n",
      "loss.mean()=tensor(1.7620, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04468320310115814\n",
      "loss.mean()=tensor(1.7513, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04698614403605461\n",
      "loss.mean()=tensor(1.7357, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.044848985970020294\n",
      "loss.mean()=tensor(1.7400, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04464241862297058\n",
      "loss.mean()=tensor(1.7456, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04368406534194946\n",
      "loss.mean()=tensor(1.7333, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.7333135604858398\n",
      "ep 39\n",
      "---------------------\n",
      "outputs.mean().item()=0.04509388655424118\n",
      "loss.mean()=tensor(1.7206, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04149789363145828\n",
      "loss.mean()=tensor(1.7212, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0427459217607975\n",
      "loss.mean()=tensor(1.7072, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04665158689022064\n",
      "loss.mean()=tensor(1.7216, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.047410860657691956\n",
      "loss.mean()=tensor(1.7384, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04731762781739235\n",
      "loss.mean()=tensor(1.6932, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.045092783868312836\n",
      "loss.mean()=tensor(1.7201, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.047226522117853165\n",
      "loss.mean()=tensor(1.7087, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04974713921546936\n",
      "loss.mean()=tensor(1.6930, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04739796370267868\n",
      "loss.mean()=tensor(1.6966, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04717693477869034\n",
      "loss.mean()=tensor(1.7017, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04612524434924126\n",
      "loss.mean()=tensor(1.6910, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.6909570693969727\n",
      "ep 40\n",
      "---------------------\n",
      "outputs.mean().item()=0.0476430244743824\n",
      "loss.mean()=tensor(1.6765, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.043692123144865036\n",
      "loss.mean()=tensor(1.6771, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.045034896582365036\n",
      "loss.mean()=tensor(1.6630, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.049321867525577545\n",
      "loss.mean()=tensor(1.6786, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.050158023834228516\n",
      "loss.mean()=tensor(1.6977, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05000950023531914\n",
      "loss.mean()=tensor(1.6486, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04759056121110916\n",
      "loss.mean()=tensor(1.6783, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04989997670054436\n",
      "loss.mean()=tensor(1.6662, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05265621095895767\n",
      "loss.mean()=tensor(1.6506, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.050079621374607086\n",
      "loss.mean()=tensor(1.6534, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.049838654696941376\n",
      "loss.mean()=tensor(1.6579, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04868442565202713\n",
      "loss.mean()=tensor(1.6491, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.6491104364395142\n",
      "ep 41\n",
      "---------------------\n",
      "outputs.mean().item()=0.05030246451497078\n",
      "loss.mean()=tensor(1.6330, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04597388952970505\n",
      "loss.mean()=tensor(1.6337, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.047412049025297165\n",
      "loss.mean()=tensor(1.6193, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.052118755877017975\n",
      "loss.mean()=tensor(1.6362, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05302237719297409\n",
      "loss.mean()=tensor(1.6575, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05284272879362106\n",
      "loss.mean()=tensor(1.6046, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05020890384912491\n",
      "loss.mean()=tensor(1.6371, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05268033593893051\n",
      "loss.mean()=tensor(1.6244, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.055691350251436234\n",
      "loss.mean()=tensor(1.6091, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05286317318677902\n",
      "loss.mean()=tensor(1.6110, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.052599601447582245\n",
      "loss.mean()=tensor(1.6147, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.051317937672138214\n",
      "loss.mean()=tensor(1.6083, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.6082690954208374\n",
      "ep 42\n",
      "---------------------\n",
      "outputs.mean().item()=0.05301713943481445\n",
      "loss.mean()=tensor(1.5904, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.048310741782188416\n",
      "loss.mean()=tensor(1.5916, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.04984680190682411\n",
      "loss.mean()=tensor(1.5768, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.054969750344753265\n",
      "loss.mean()=tensor(1.5949, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05592558532953262\n",
      "loss.mean()=tensor(1.6183, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05568123981356621\n",
      "loss.mean()=tensor(1.5619, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05284746736288071\n",
      "loss.mean()=tensor(1.5970, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.055477552115917206\n",
      "loss.mean()=tensor(1.5839, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.058742839843034744\n",
      "loss.mean()=tensor(1.5690, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0556529276072979\n",
      "loss.mean()=tensor(1.5697, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05537049099802971\n",
      "loss.mean()=tensor(1.5725, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05398973450064659\n",
      "loss.mean()=tensor(1.5688, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.5688352584838867\n",
      "ep 43\n",
      "---------------------\n",
      "outputs.mean().item()=0.055774129927158356\n",
      "loss.mean()=tensor(1.5494, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05068235471844673\n",
      "loss.mean()=tensor(1.5510, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05231567472219467\n",
      "loss.mean()=tensor(1.5357, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05785554647445679\n",
      "loss.mean()=tensor(1.5550, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.058883391320705414\n",
      "loss.mean()=tensor(1.5804, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05856284499168396\n",
      "loss.mean()=tensor(1.5208, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05552035570144653\n",
      "loss.mean()=tensor(1.5583, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05830056592822075\n",
      "loss.mean()=tensor(1.5449, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.061835866421461105\n",
      "loss.mean()=tensor(1.5306, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05847800895571709\n",
      "loss.mean()=tensor(1.5299, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.058175016194581985\n",
      "loss.mean()=tensor(1.5318, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05668213218450546\n",
      "loss.mean()=tensor(1.5310, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.531020164489746\n",
      "ep 44\n",
      "---------------------\n",
      "outputs.mean().item()=0.058552443981170654\n",
      "loss.mean()=tensor(1.5101, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05307776480913162\n",
      "loss.mean()=tensor(1.5124, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0548107847571373\n",
      "loss.mean()=tensor(1.4963, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06076779216527939\n",
      "loss.mean()=tensor(1.5168, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0618649497628212\n",
      "loss.mean()=tensor(1.5439, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.061475515365600586\n",
      "loss.mean()=tensor(1.4813, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05821901559829712\n",
      "loss.mean()=tensor(1.5212, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.061159610748291016\n",
      "loss.mean()=tensor(1.5075, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0649537593126297\n",
      "loss.mean()=tensor(1.4940, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06133188679814339\n",
      "loss.mean()=tensor(1.4919, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.061002396047115326\n",
      "loss.mean()=tensor(1.4927, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.059420693665742874\n",
      "loss.mean()=tensor(1.4949, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.4949175119400024\n",
      "ep 45\n",
      "---------------------\n",
      "outputs.mean().item()=0.06135525554418564\n",
      "loss.mean()=tensor(1.4726, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05551699921488762\n",
      "loss.mean()=tensor(1.4757, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.057334136217832565\n",
      "loss.mean()=tensor(1.4587, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06369228661060333\n",
      "loss.mean()=tensor(1.4803, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0648789033293724\n",
      "loss.mean()=tensor(1.5091, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06443367898464203\n",
      "loss.mean()=tensor(1.4439, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06097177788615227\n",
      "loss.mean()=tensor(1.4858, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06404732167720795\n",
      "loss.mean()=tensor(1.4719, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06811235100030899\n",
      "loss.mean()=tensor(1.4592, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06422699987888336\n",
      "loss.mean()=tensor(1.4556, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06387434154748917\n",
      "loss.mean()=tensor(1.4555, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06220652535557747\n",
      "loss.mean()=tensor(1.4606, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.4605849981307983\n",
      "ep 46\n",
      "---------------------\n",
      "outputs.mean().item()=0.064219631254673\n",
      "loss.mean()=tensor(1.4370, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05803501605987549\n",
      "loss.mean()=tensor(1.4411, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.05994725227355957\n",
      "loss.mean()=tensor(1.4231, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06670386344194412\n",
      "loss.mean()=tensor(1.4457, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06799502670764923\n",
      "loss.mean()=tensor(1.4761, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06748345494270325\n",
      "loss.mean()=tensor(1.4084, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06382428109645844\n",
      "loss.mean()=tensor(1.4523, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06703968346118927\n",
      "loss.mean()=tensor(1.4382, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07137751579284668\n",
      "loss.mean()=tensor(1.4264, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0672440156340599\n",
      "loss.mean()=tensor(1.4213, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06689150631427765\n",
      "loss.mean()=tensor(1.4203, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06512794643640518\n",
      "loss.mean()=tensor(1.4280, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.4280179738998413\n",
      "ep 47\n",
      "---------------------\n",
      "outputs.mean().item()=0.06720715761184692\n",
      "loss.mean()=tensor(1.4033, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.060684867203235626\n",
      "loss.mean()=tensor(1.4085, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06267103552818298\n",
      "loss.mean()=tensor(1.3894, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06982167065143585\n",
      "loss.mean()=tensor(1.4129, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0712127685546875\n",
      "loss.mean()=tensor(1.4448, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07065682113170624\n",
      "loss.mean()=tensor(1.3750, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06679601222276688\n",
      "loss.mean()=tensor(1.4205, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07015024125576019\n",
      "loss.mean()=tensor(1.4063, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07475309073925018\n",
      "loss.mean()=tensor(1.3954, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07037070393562317\n",
      "loss.mean()=tensor(1.3888, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06998512893915176\n",
      "loss.mean()=tensor(1.3870, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06812041252851486\n",
      "loss.mean()=tensor(1.3972, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.3971651792526245\n",
      "ep 48\n",
      "---------------------\n",
      "outputs.mean().item()=0.07027594745159149\n",
      "loss.mean()=tensor(1.3716, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06343604624271393\n",
      "loss.mean()=tensor(1.3779, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06549318134784698\n",
      "loss.mean()=tensor(1.3575, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07301972806453705\n",
      "loss.mean()=tensor(1.3819, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07451455295085907\n",
      "loss.mean()=tensor(1.4153, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.073916494846344\n",
      "loss.mean()=tensor(1.3434, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06985475867986679\n",
      "loss.mean()=tensor(1.3905, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07334712892770767\n",
      "loss.mean()=tensor(1.3761, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07822902500629425\n",
      "loss.mean()=tensor(1.3662, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07361239194869995\n",
      "loss.mean()=tensor(1.3581, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07321010529994965\n",
      "loss.mean()=tensor(1.3557, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0712672546505928\n",
      "loss.mean()=tensor(1.3679, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.367933988571167\n",
      "ep 49\n",
      "---------------------\n",
      "outputs.mean().item()=0.07349735498428345\n",
      "loss.mean()=tensor(1.3416, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06635527312755585\n",
      "loss.mean()=tensor(1.3492, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06851015985012054\n",
      "loss.mean()=tensor(1.3275, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0764046162366867\n",
      "loss.mean()=tensor(1.3526, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07799128443002701\n",
      "loss.mean()=tensor(1.3874, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07735269516706467\n",
      "loss.mean()=tensor(1.3138, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07306452095508575\n",
      "loss.mean()=tensor(1.3621, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0766942948102951\n",
      "loss.mean()=tensor(1.3476, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08184096217155457\n",
      "loss.mean()=tensor(1.3386, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07700725644826889\n",
      "loss.mean()=tensor(1.3292, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07657177746295929\n",
      "loss.mean()=tensor(1.3262, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07452375441789627\n",
      "loss.mean()=tensor(1.3402, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.3402372598648071\n",
      "ep 50\n",
      "---------------------\n",
      "outputs.mean().item()=0.07684089243412018\n",
      "loss.mean()=tensor(1.3132, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.06937861442565918\n",
      "loss.mean()=tensor(1.3223, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0716240182518959\n",
      "loss.mean()=tensor(1.2990, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07986392825841904\n",
      "loss.mean()=tensor(1.3249, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0815344825387001\n",
      "loss.mean()=tensor(1.3612, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0808393582701683\n",
      "loss.mean()=tensor(1.2859, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07633648812770844\n",
      "loss.mean()=tensor(1.3353, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08011789619922638\n",
      "loss.mean()=tensor(1.3207, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08550693839788437\n",
      "loss.mean()=tensor(1.3127, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08045642822980881\n",
      "loss.mean()=tensor(1.3019, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07997316867113113\n",
      "loss.mean()=tensor(1.2984, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07783564925193787\n",
      "loss.mean()=tensor(1.3140, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.313987374305725\n",
      "ep 51\n",
      "---------------------\n",
      "outputs.mean().item()=0.08018374443054199\n",
      "loss.mean()=tensor(1.2865, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07239736616611481\n",
      "loss.mean()=tensor(1.2970, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0747271180152893\n",
      "loss.mean()=tensor(1.2722, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08331868797540665\n",
      "loss.mean()=tensor(1.2987, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08507587015628815\n",
      "loss.mean()=tensor(1.3364, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08432253450155258\n",
      "loss.mean()=tensor(1.2596, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07961633801460266\n",
      "loss.mean()=tensor(1.3100, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0835135281085968\n",
      "loss.mean()=tensor(1.2952, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08912532031536102\n",
      "loss.mean()=tensor(1.2882, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08386602997779846\n",
      "loss.mean()=tensor(1.2762, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08334742486476898\n",
      "loss.mean()=tensor(1.2721, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08112909644842148\n",
      "loss.mean()=tensor(1.2891, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.2891157865524292\n",
      "ep 52\n",
      "---------------------\n",
      "outputs.mean().item()=0.0835392102599144\n",
      "loss.mean()=tensor(1.2613, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07544462382793427\n",
      "loss.mean()=tensor(1.2731, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07786409556865692\n",
      "loss.mean()=tensor(1.2468, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08680125325918198\n",
      "loss.mean()=tensor(1.2739, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08861076086759567\n",
      "loss.mean()=tensor(1.3131, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08780963718891144\n",
      "loss.mean()=tensor(1.2348, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08291815966367722\n",
      "loss.mean()=tensor(1.2861, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08691282570362091\n",
      "loss.mean()=tensor(1.2711, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09275052696466446\n",
      "loss.mean()=tensor(1.2651, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0872831717133522\n",
      "loss.mean()=tensor(1.2518, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0867423340678215\n",
      "loss.mean()=tensor(1.2474, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08442696183919907\n",
      "loss.mean()=tensor(1.2655, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.2655473947525024\n",
      "ep 53\n",
      "---------------------\n",
      "outputs.mean().item()=0.08688616752624512\n",
      "loss.mean()=tensor(1.2374, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.07849415391683578\n",
      "loss.mean()=tensor(1.2506, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08098317682743073\n",
      "loss.mean()=tensor(1.2228, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09022115170955658\n",
      "loss.mean()=tensor(1.2504, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09211448580026627\n",
      "loss.mean()=tensor(1.2911, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09127143770456314\n",
      "loss.mean()=tensor(1.2114, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08617862313985825\n",
      "loss.mean()=tensor(1.2634, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09025780856609344\n",
      "loss.mean()=tensor(1.2484, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09629560261964798\n",
      "loss.mean()=tensor(1.2432, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09064735472202301\n",
      "loss.mean()=tensor(1.2288, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09004970639944077\n",
      "loss.mean()=tensor(1.2239, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08765492588281631\n",
      "loss.mean()=tensor(1.2432, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.2431727647781372\n",
      "ep 54\n",
      "---------------------\n",
      "outputs.mean().item()=0.09015439450740814\n",
      "loss.mean()=tensor(1.2149, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08145185559988022\n",
      "loss.mean()=tensor(1.2293, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08401121944189072\n",
      "loss.mean()=tensor(1.1999, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09357478469610214\n",
      "loss.mean()=tensor(1.2281, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09556152671575546\n",
      "loss.mean()=tensor(1.2703, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09468358755111694\n",
      "loss.mean()=tensor(1.1892, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08941364288330078\n",
      "loss.mean()=tensor(1.2419, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09357468038797379\n",
      "loss.mean()=tensor(1.2268, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0998140424489975\n",
      "loss.mean()=tensor(1.2225, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09397491812705994\n",
      "loss.mean()=tensor(1.2070, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09334024041891098\n",
      "loss.mean()=tensor(1.2017, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09085331857204437\n",
      "loss.mean()=tensor(1.2219, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.2219111919403076\n",
      "ep 55\n",
      "---------------------\n",
      "outputs.mean().item()=0.0934157744050026\n",
      "loss.mean()=tensor(1.1935, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0844268947839737\n",
      "loss.mean()=tensor(1.2092, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08706218004226685\n",
      "loss.mean()=tensor(1.1783, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09692742675542831\n",
      "loss.mean()=tensor(1.2068, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09899424761533737\n",
      "loss.mean()=tensor(1.2506, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09808509051799774\n",
      "loss.mean()=tensor(1.1682, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09265822172164917\n",
      "loss.mean()=tensor(1.2214, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09690286964178085\n",
      "loss.mean()=tensor(1.2063, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10334118455648422\n",
      "loss.mean()=tensor(1.2029, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09733327478170395\n",
      "loss.mean()=tensor(1.1862, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09664922952651978\n",
      "loss.mean()=tensor(1.1805, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09409169852733612\n",
      "loss.mean()=tensor(1.2017, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.2016990184783936\n",
      "ep 56\n",
      "---------------------\n",
      "outputs.mean().item()=0.09670182317495346\n",
      "loss.mean()=tensor(1.1732, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.08743961900472641\n",
      "loss.mean()=tensor(1.1901, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0901547372341156\n",
      "loss.mean()=tensor(1.1577, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10032975673675537\n",
      "loss.mean()=tensor(1.1866, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10246138274669647\n",
      "loss.mean()=tensor(1.2319, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10147205740213394\n",
      "loss.mean()=tensor(1.1482, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09590284526348114\n",
      "loss.mean()=tensor(1.2019, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10023564100265503\n",
      "loss.mean()=tensor(1.1868, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10687470436096191\n",
      "loss.mean()=tensor(1.1842, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1006837859749794\n",
      "loss.mean()=tensor(1.1665, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0999584048986435\n",
      "loss.mean()=tensor(1.1604, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09735642373561859\n",
      "loss.mean()=tensor(1.1825, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.182479739189148\n",
      "ep 57\n",
      "---------------------\n",
      "outputs.mean().item()=0.10000069439411163\n",
      "loss.mean()=tensor(1.1539, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09050453454256058\n",
      "loss.mean()=tensor(1.1720, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09327153861522675\n",
      "loss.mean()=tensor(1.1382, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10372886806726456\n",
      "loss.mean()=tensor(1.1674, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10593614727258682\n",
      "loss.mean()=tensor(1.2142, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10488533973693848\n",
      "loss.mean()=tensor(1.1292, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.0991944894194603\n",
      "loss.mean()=tensor(1.1833, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10360300540924072\n",
      "loss.mean()=tensor(1.1684, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11047365516424179\n",
      "loss.mean()=tensor(1.1665, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10410189628601074\n",
      "loss.mean()=tensor(1.1478, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10333774983882904\n",
      "loss.mean()=tensor(1.1414, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10069907456636429\n",
      "loss.mean()=tensor(1.1642, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.1641929149627686\n",
      "ep 58\n",
      "---------------------\n",
      "outputs.mean().item()=0.10337875038385391\n",
      "loss.mean()=tensor(1.1356, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09365721046924591\n",
      "loss.mean()=tensor(1.1549, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09651894867420197\n",
      "loss.mean()=tensor(1.1196, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10725706815719604\n",
      "loss.mean()=tensor(1.1490, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10954225063323975\n",
      "loss.mean()=tensor(1.1975, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10840461403131485\n",
      "loss.mean()=tensor(1.1113, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10259704291820526\n",
      "loss.mean()=tensor(1.1657, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10707602649927139\n",
      "loss.mean()=tensor(1.1509, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11417516320943832\n",
      "loss.mean()=tensor(1.1497, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10767271369695663\n",
      "loss.mean()=tensor(1.1301, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10687122493982315\n",
      "loss.mean()=tensor(1.1232, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10417679697275162\n",
      "loss.mean()=tensor(1.1468, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.146772861480713\n",
      "ep 59\n",
      "---------------------\n",
      "outputs.mean().item()=0.10686580836772919\n",
      "loss.mean()=tensor(1.1183, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09694256633520126\n",
      "loss.mean()=tensor(1.1388, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.09988470375537872\n",
      "loss.mean()=tensor(1.1019, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11086873710155487\n",
      "loss.mean()=tensor(1.1316, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11325019598007202\n",
      "loss.mean()=tensor(1.1816, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11203373968601227\n",
      "loss.mean()=tensor(1.0942, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1061105728149414\n",
      "loss.mean()=tensor(1.1490, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1106385737657547\n",
      "loss.mean()=tensor(1.1342, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11794175952672958\n",
      "loss.mean()=tensor(1.1338, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11129806190729141\n",
      "loss.mean()=tensor(1.1132, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11046228557825089\n",
      "loss.mean()=tensor(1.1060, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10772388428449631\n",
      "loss.mean()=tensor(1.1302, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.1301872730255127\n",
      "ep 60\n",
      "---------------------\n",
      "outputs.mean().item()=0.11043329536914825\n",
      "loss.mean()=tensor(1.1018, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10029858350753784\n",
      "loss.mean()=tensor(1.1234, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10332322120666504\n",
      "loss.mean()=tensor(1.0851, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11453931033611298\n",
      "loss.mean()=tensor(1.1149, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11700601875782013\n",
      "loss.mean()=tensor(1.1665, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11569809913635254\n",
      "loss.mean()=tensor(1.0779, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10968425124883652\n",
      "loss.mean()=tensor(1.1330, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11424513906240463\n",
      "loss.mean()=tensor(1.1184, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12174447625875473\n",
      "loss.mean()=tensor(1.1186, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1149461418390274\n",
      "loss.mean()=tensor(1.0971, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11406826972961426\n",
      "loss.mean()=tensor(1.0896, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11128923296928406\n",
      "loss.mean()=tensor(1.1144, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.114386796951294\n",
      "ep 61\n",
      "---------------------\n",
      "outputs.mean().item()=0.1140245646238327\n",
      "loss.mean()=tensor(1.0862, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10369336605072021\n",
      "loss.mean()=tensor(1.1088, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10678525269031525\n",
      "loss.mean()=tensor(1.0691, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11824238300323486\n",
      "loss.mean()=tensor(1.0990, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1207689419388771\n",
      "loss.mean()=tensor(1.1522, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11936436593532562\n",
      "loss.mean()=tensor(1.0625, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11328180134296417\n",
      "loss.mean()=tensor(1.1177, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11786170303821564\n",
      "loss.mean()=tensor(1.1033, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1255580484867096\n",
      "loss.mean()=tensor(1.1041, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11860841512680054\n",
      "loss.mean()=tensor(1.0818, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11766761541366577\n",
      "loss.mean()=tensor(1.0740, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11484276503324509\n",
      "loss.mean()=tensor(1.0993, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.0993125438690186\n",
      "ep 62\n",
      "---------------------\n",
      "outputs.mean().item()=0.11757566034793854\n",
      "loss.mean()=tensor(1.0713, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.10707525908946991\n",
      "loss.mean()=tensor(1.0950, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11024495214223862\n",
      "loss.mean()=tensor(1.0539, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12191440910100937\n",
      "loss.mean()=tensor(1.0838, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12452635914087296\n",
      "loss.mean()=tensor(1.1385, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1230238676071167\n",
      "loss.mean()=tensor(1.0478, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11688277870416641\n",
      "loss.mean()=tensor(1.1032, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12149707973003387\n",
      "loss.mean()=tensor(1.0890, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12935924530029297\n",
      "loss.mean()=tensor(1.0904, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12226833403110504\n",
      "loss.mean()=tensor(1.0673, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12127913534641266\n",
      "loss.mean()=tensor(1.0591, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11842099577188492\n",
      "loss.mean()=tensor(1.0849, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.0849381685256958\n",
      "ep 63\n",
      "---------------------\n",
      "outputs.mean().item()=0.12114839255809784\n",
      "loss.mean()=tensor(1.0571, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11046390235424042\n",
      "loss.mean()=tensor(1.0818, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11369079351425171\n",
      "loss.mean()=tensor(1.0393, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12557363510131836\n",
      "loss.mean()=tensor(1.0692, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12827718257904053\n",
      "loss.mean()=tensor(1.1255, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12668009102344513\n",
      "loss.mean()=tensor(1.0339, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12045924365520477\n",
      "loss.mean()=tensor(1.0893, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1250803917646408\n",
      "loss.mean()=tensor(1.0753, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13308747112751007\n",
      "loss.mean()=tensor(1.0772, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12586036324501038\n",
      "loss.mean()=tensor(1.0534, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12481341511011124\n",
      "loss.mean()=tensor(1.0449, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12193174660205841\n",
      "loss.mean()=tensor(1.0712, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.071192979812622\n",
      "ep 64\n",
      "---------------------\n",
      "outputs.mean().item()=0.12464729696512222\n",
      "loss.mean()=tensor(1.0436, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11380021274089813\n",
      "loss.mean()=tensor(1.0692, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1171046644449234\n",
      "loss.mean()=tensor(1.0254, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12920954823493958\n",
      "loss.mean()=tensor(1.0552, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13200156390666962\n",
      "loss.mean()=tensor(1.1131, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13029047846794128\n",
      "loss.mean()=tensor(1.0205, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12399008125066757\n",
      "loss.mean()=tensor(1.0760, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12861759960651398\n",
      "loss.mean()=tensor(1.0622, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1367965191602707\n",
      "loss.mean()=tensor(1.0646, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12945504486560822\n",
      "loss.mean()=tensor(1.0401, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12834224104881287\n",
      "loss.mean()=tensor(1.0313, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12541912496089935\n",
      "loss.mean()=tensor(1.0580, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.0580475330352783\n",
      "ep 65\n",
      "---------------------\n",
      "outputs.mean().item()=0.1280948668718338\n",
      "loss.mean()=tensor(1.0307, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.11710800975561142\n",
      "loss.mean()=tensor(1.0572, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12049613147974014\n",
      "loss.mean()=tensor(1.0121, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13278257846832275\n",
      "loss.mean()=tensor(1.0418, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1356305629014969\n",
      "loss.mean()=tensor(1.1012, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13380548357963562\n",
      "loss.mean()=tensor(1.0078, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1274636685848236\n",
      "loss.mean()=tensor(1.0633, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13206128776073456\n",
      "loss.mean()=tensor(1.0498, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14036904275417328\n",
      "loss.mean()=tensor(1.0526, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13294020295143127\n",
      "loss.mean()=tensor(1.0274, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13176235556602478\n",
      "loss.mean()=tensor(1.0183, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1288144290447235\n",
      "loss.mean()=tensor(1.0454, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.045448660850525\n",
      "ep 66\n",
      "---------------------\n",
      "outputs.mean().item()=0.1314898431301117\n",
      "loss.mean()=tensor(1.0184, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12034056335687637\n",
      "loss.mean()=tensor(1.0458, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12379658222198486\n",
      "loss.mean()=tensor(0.9994, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13629314303398132\n",
      "loss.mean()=tensor(1.0289, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13921549916267395\n",
      "loss.mean()=tensor(1.0899, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13727295398712158\n",
      "loss.mean()=tensor(0.9957, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13087543845176697\n",
      "loss.mean()=tensor(1.0511, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13544148206710815\n",
      "loss.mean()=tensor(1.0379, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14388588070869446\n",
      "loss.mean()=tensor(1.0411, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13635548949241638\n",
      "loss.mean()=tensor(1.0153, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1351427137851715\n",
      "loss.mean()=tensor(1.0059, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13217809796333313\n",
      "loss.mean()=tensor(1.0333, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.0333125591278076\n",
      "ep 67\n",
      "---------------------\n",
      "outputs.mean().item()=0.1348399817943573\n",
      "loss.mean()=tensor(1.0065, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12354230880737305\n",
      "loss.mean()=tensor(1.0348, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.127073734998703\n",
      "loss.mean()=tensor(0.9872, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13975588977336884\n",
      "loss.mean()=tensor(1.0165, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14275726675987244\n",
      "loss.mean()=tensor(1.0790, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1406985968351364\n",
      "loss.mean()=tensor(0.9840, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13425970077514648\n",
      "loss.mean()=tensor(1.0394, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13878539204597473\n",
      "loss.mean()=tensor(1.0264, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14734387397766113\n",
      "loss.mean()=tensor(1.0300, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13971684873104095\n",
      "loss.mean()=tensor(1.0036, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13844409584999084\n",
      "loss.mean()=tensor(0.9939, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1354539692401886\n",
      "loss.mean()=tensor(1.0216, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.0216381549835205\n",
      "ep 68\n",
      "---------------------\n",
      "outputs.mean().item()=0.1381036937236786\n",
      "loss.mean()=tensor(0.9951, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12666025757789612\n",
      "loss.mean()=tensor(1.0243, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1302894651889801\n",
      "loss.mean()=tensor(0.9755, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14312875270843506\n",
      "loss.mean()=tensor(1.0045, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14621078968048096\n",
      "loss.mean()=tensor(1.0685, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14404341578483582\n",
      "loss.mean()=tensor(0.9729, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1375596821308136\n",
      "loss.mean()=tensor(1.0281, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14204420149326324\n",
      "loss.mean()=tensor(1.0154, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15074320137500763\n",
      "loss.mean()=tensor(1.0193, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14301732182502747\n",
      "loss.mean()=tensor(0.9924, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14168816804885864\n",
      "loss.mean()=tensor(0.9824, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13867706060409546\n",
      "loss.mean()=tensor(1.0105, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=1.0104608535766602\n",
      "ep 69\n",
      "---------------------\n",
      "outputs.mean().item()=0.14132246375083923\n",
      "loss.mean()=tensor(0.9843, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.12975510954856873\n",
      "loss.mean()=tensor(1.0142, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1334819793701172\n",
      "loss.mean()=tensor(0.9643, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14648090302944183\n",
      "loss.mean()=tensor(0.9930, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.149640291929245\n",
      "loss.mean()=tensor(1.0585, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1473560333251953\n",
      "loss.mean()=tensor(0.9622, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1408291757106781\n",
      "loss.mean()=tensor(1.0173, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14527885615825653\n",
      "loss.mean()=tensor(1.0049, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15412548184394836\n",
      "loss.mean()=tensor(1.0092, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14630544185638428\n",
      "loss.mean()=tensor(0.9818, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14492180943489075\n",
      "loss.mean()=tensor(0.9714, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14188580214977264\n",
      "loss.mean()=tensor(0.9998, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9997588992118835\n",
      "ep 70\n",
      "---------------------\n",
      "outputs.mean().item()=0.14449867606163025\n",
      "loss.mean()=tensor(0.9739, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.132799431681633\n",
      "loss.mean()=tensor(1.0047, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13656076788902283\n",
      "loss.mean()=tensor(0.9536, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1497180014848709\n",
      "loss.mean()=tensor(0.9819, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15299758315086365\n",
      "loss.mean()=tensor(1.0490, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15061336755752563\n",
      "loss.mean()=tensor(0.9521, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1440599411725998\n",
      "loss.mean()=tensor(1.0069, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1484534740447998\n",
      "loss.mean()=tensor(0.9949, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1574312001466751\n",
      "loss.mean()=tensor(0.9995, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1495080590248108\n",
      "loss.mean()=tensor(0.9716, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14804896712303162\n",
      "loss.mean()=tensor(0.9609, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14499907195568085\n",
      "loss.mean()=tensor(0.9895, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9895318150520325\n",
      "ep 71\n",
      "---------------------\n",
      "outputs.mean().item()=0.14758877456188202\n",
      "loss.mean()=tensor(0.9640, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.13579826056957245\n",
      "loss.mean()=tensor(0.9956, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1396089345216751\n",
      "loss.mean()=tensor(0.9434, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15287968516349792\n",
      "loss.mean()=tensor(0.9713, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15622463822364807\n",
      "loss.mean()=tensor(1.0398, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15371863543987274\n",
      "loss.mean()=tensor(0.9425, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14712533354759216\n",
      "loss.mean()=tensor(0.9970, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15148921310901642\n",
      "loss.mean()=tensor(0.9854, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16058632731437683\n",
      "loss.mean()=tensor(0.9902, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15258987247943878\n",
      "loss.mean()=tensor(0.9618, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1510542780160904\n",
      "loss.mean()=tensor(0.9508, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14799892902374268\n",
      "loss.mean()=tensor(0.9797, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9797254800796509\n",
      "ep 72\n",
      "---------------------\n",
      "outputs.mean().item()=0.15056321024894714\n",
      "loss.mean()=tensor(0.9546, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1386551856994629\n",
      "loss.mean()=tensor(0.9869, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14254353940486908\n",
      "loss.mean()=tensor(0.9336, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1559302806854248\n",
      "loss.mean()=tensor(0.9611, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15933741629123688\n",
      "loss.mean()=tensor(1.0310, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15671923756599426\n",
      "loss.mean()=tensor(0.9333, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15010440349578857\n",
      "loss.mean()=tensor(0.9875, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15441636741161346\n",
      "loss.mean()=tensor(0.9763, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16364498436450958\n",
      "loss.mean()=tensor(0.9813, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1555827558040619\n",
      "loss.mean()=tensor(0.9525, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1539624035358429\n",
      "loss.mean()=tensor(0.9412, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15090109407901764\n",
      "loss.mean()=tensor(0.9703, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9703226685523987\n",
      "ep 73\n",
      "---------------------\n",
      "outputs.mean().item()=0.15344645082950592\n",
      "loss.mean()=tensor(0.9456, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14143255352973938\n",
      "loss.mean()=tensor(0.9785, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14541468024253845\n",
      "loss.mean()=tensor(0.9242, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15893514454364777\n",
      "loss.mean()=tensor(0.9513, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16239210963249207\n",
      "loss.mean()=tensor(1.0226, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1596711128950119\n",
      "loss.mean()=tensor(0.9245, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15303179621696472\n",
      "loss.mean()=tensor(0.9784, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15731486678123474\n",
      "loss.mean()=tensor(0.9676, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16666413843631744\n",
      "loss.mean()=tensor(0.9727, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1584915816783905\n",
      "loss.mean()=tensor(0.9435, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15682728588581085\n",
      "loss.mean()=tensor(0.9320, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15377993881702423\n",
      "loss.mean()=tensor(0.9613, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9612865447998047\n",
      "ep 74\n",
      "---------------------\n",
      "outputs.mean().item()=0.15630671381950378\n",
      "loss.mean()=tensor(0.9369, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1441742479801178\n",
      "loss.mean()=tensor(0.9706, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1482294797897339\n",
      "loss.mean()=tensor(0.9152, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16186633706092834\n",
      "loss.mean()=tensor(0.9419, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1653943806886673\n",
      "loss.mean()=tensor(1.0145, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16258004307746887\n",
      "loss.mean()=tensor(0.9160, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15590795874595642\n",
      "loss.mean()=tensor(0.9696, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16015180945396423\n",
      "loss.mean()=tensor(0.9592, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1695922166109085\n",
      "loss.mean()=tensor(0.9645, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1613415777683258\n",
      "loss.mean()=tensor(0.9350, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15962858498096466\n",
      "loss.mean()=tensor(0.9231, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1565973311662674\n",
      "loss.mean()=tensor(0.9526, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9526042342185974\n",
      "ep 75\n",
      "---------------------\n",
      "outputs.mean().item()=0.1591303050518036\n",
      "loss.mean()=tensor(0.9286, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14691485464572906\n",
      "loss.mean()=tensor(0.9629, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15102891623973846\n",
      "loss.mean()=tensor(0.9065, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16476082801818848\n",
      "loss.mean()=tensor(0.9327, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16834335029125214\n",
      "loss.mean()=tensor(1.0067, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16541631519794464\n",
      "loss.mean()=tensor(0.9079, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1587168425321579\n",
      "loss.mean()=tensor(0.9612, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16293510794639587\n",
      "loss.mean()=tensor(0.9512, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17248353362083435\n",
      "loss.mean()=tensor(0.9567, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16414058208465576\n",
      "loss.mean()=tensor(0.9267, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1623549461364746\n",
      "loss.mean()=tensor(0.9145, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15934446454048157\n",
      "loss.mean()=tensor(0.9442, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9442434906959534\n",
      "ep 76\n",
      "---------------------\n",
      "outputs.mean().item()=0.1618524044752121\n",
      "loss.mean()=tensor(0.9206, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.14955151081085205\n",
      "loss.mean()=tensor(0.9556, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15373942255973816\n",
      "loss.mean()=tensor(0.8982, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16756002604961395\n",
      "loss.mean()=tensor(0.9240, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17118589580059052\n",
      "loss.mean()=tensor(0.9992, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1681348830461502\n",
      "loss.mean()=tensor(0.9002, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16142535209655762\n",
      "loss.mean()=tensor(0.9530, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16561779379844666\n",
      "loss.mean()=tensor(0.9436, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17525434494018555\n",
      "loss.mean()=tensor(0.9491, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16685862839221954\n",
      "loss.mean()=tensor(0.9188, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16501502692699432\n",
      "loss.mean()=tensor(0.9063, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16203752160072327\n",
      "loss.mean()=tensor(0.9362, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9361961483955383\n",
      "ep 77\n",
      "---------------------\n",
      "outputs.mean().item()=0.16452500224113464\n",
      "loss.mean()=tensor(0.9129, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15213680267333984\n",
      "loss.mean()=tensor(0.9485, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.156389981508255\n",
      "loss.mean()=tensor(0.8902, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1703159213066101\n",
      "loss.mean()=tensor(0.9155, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17400890588760376\n",
      "loss.mean()=tensor(0.9919, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17085352540016174\n",
      "loss.mean()=tensor(0.8927, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16413119435310364\n",
      "loss.mean()=tensor(0.9452, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16828219592571259\n",
      "loss.mean()=tensor(0.9362, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1780138909816742\n",
      "loss.mean()=tensor(0.9417, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1695745438337326\n",
      "loss.mean()=tensor(0.9112, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1676618754863739\n",
      "loss.mean()=tensor(0.8984, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1647142916917801\n",
      "loss.mean()=tensor(0.9284, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9284352660179138\n",
      "ep 78\n",
      "---------------------\n",
      "outputs.mean().item()=0.1671438217163086\n",
      "loss.mean()=tensor(0.9055, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15468232333660126\n",
      "loss.mean()=tensor(0.9417, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1589888036251068\n",
      "loss.mean()=tensor(0.8825, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17300152778625488\n",
      "loss.mean()=tensor(0.9073, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1767520159482956\n",
      "loss.mean()=tensor(0.9849, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17349910736083984\n",
      "loss.mean()=tensor(0.8856, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16675713658332825\n",
      "loss.mean()=tensor(0.9377, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1708524525165558\n",
      "loss.mean()=tensor(0.9291, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1806880235671997\n",
      "loss.mean()=tensor(0.9347, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17219090461730957\n",
      "loss.mean()=tensor(0.9038, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17019245028495789\n",
      "loss.mean()=tensor(0.8907, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16726180911064148\n",
      "loss.mean()=tensor(0.9210, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9209532737731934\n",
      "ep 79\n",
      "---------------------\n",
      "outputs.mean().item()=0.16968338191509247\n",
      "loss.mean()=tensor(0.8984, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15714579820632935\n",
      "loss.mean()=tensor(0.9352, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16150934994220734\n",
      "loss.mean()=tensor(0.8751, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1756160706281662\n",
      "loss.mean()=tensor(0.8993, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1794382780790329\n",
      "loss.mean()=tensor(0.9781, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17608492076396942\n",
      "loss.mean()=tensor(0.8787, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.169346421957016\n",
      "loss.mean()=tensor(0.9304, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1733899712562561\n",
      "loss.mean()=tensor(0.9223, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18327119946479797\n",
      "loss.mean()=tensor(0.9279, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17474158108234406\n",
      "loss.mean()=tensor(0.8968, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17269645631313324\n",
      "loss.mean()=tensor(0.8834, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16979703307151794\n",
      "loss.mean()=tensor(0.9137, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9137367010116577\n",
      "ep 80\n",
      "---------------------\n",
      "outputs.mean().item()=0.17217326164245605\n",
      "loss.mean()=tensor(0.8916, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.15955203771591187\n",
      "loss.mean()=tensor(0.9289, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1639556884765625\n",
      "loss.mean()=tensor(0.8679, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17815104126930237\n",
      "loss.mean()=tensor(0.8917, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18205800652503967\n",
      "loss.mean()=tensor(0.9716, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17860761284828186\n",
      "loss.mean()=tensor(0.8722, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17186608910560608\n",
      "loss.mean()=tensor(0.9234, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17585457861423492\n",
      "loss.mean()=tensor(0.9158, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18582284450531006\n",
      "loss.mean()=tensor(0.9213, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17725196480751038\n",
      "loss.mean()=tensor(0.8899, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17514148354530334\n",
      "loss.mean()=tensor(0.8762, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17224907875061035\n",
      "loss.mean()=tensor(0.9068, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9067649841308594\n",
      "ep 81\n",
      "---------------------\n",
      "outputs.mean().item()=0.17459455132484436\n",
      "loss.mean()=tensor(0.8850, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1619115024805069\n",
      "loss.mean()=tensor(0.9229, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16637663543224335\n",
      "loss.mean()=tensor(0.8610, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18062512576580048\n",
      "loss.mean()=tensor(0.8842, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18461187183856964\n",
      "loss.mean()=tensor(0.9653, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18106822669506073\n",
      "loss.mean()=tensor(0.8658, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17434637248516083\n",
      "loss.mean()=tensor(0.9166, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1782732605934143\n",
      "loss.mean()=tensor(0.9095, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18831419944763184\n",
      "loss.mean()=tensor(0.9150, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17969492077827454\n",
      "loss.mean()=tensor(0.8834, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1775140017271042\n",
      "loss.mean()=tensor(0.8694, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17465844750404358\n",
      "loss.mean()=tensor(0.9000, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.9000333547592163\n",
      "ep 82\n",
      "---------------------\n",
      "outputs.mean().item()=0.17698651552200317\n",
      "loss.mean()=tensor(0.8786, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16424629092216492\n",
      "loss.mean()=tensor(0.9170, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1687551736831665\n",
      "loss.mean()=tensor(0.8543, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18305595219135284\n",
      "loss.mean()=tensor(0.8771, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18713250756263733\n",
      "loss.mean()=tensor(0.9592, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18350841104984283\n",
      "loss.mean()=tensor(0.8597, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17677366733551025\n",
      "loss.mean()=tensor(0.9100, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18064488470554352\n",
      "loss.mean()=tensor(0.9035, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19077007472515106\n",
      "loss.mean()=tensor(0.9089, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18208837509155273\n",
      "loss.mean()=tensor(0.8770, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17986008524894714\n",
      "loss.mean()=tensor(0.8627, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17704425752162933\n",
      "loss.mean()=tensor(0.8935, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.893523097038269\n",
      "ep 83\n",
      "---------------------\n",
      "outputs.mean().item()=0.17933081090450287\n",
      "loss.mean()=tensor(0.8725, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16654320061206818\n",
      "loss.mean()=tensor(0.9114, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1711152344942093\n",
      "loss.mean()=tensor(0.8479, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1854519248008728\n",
      "loss.mean()=tensor(0.8701, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.189595565199852\n",
      "loss.mean()=tensor(0.9533, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18585994839668274\n",
      "loss.mean()=tensor(0.8539, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17915795743465424\n",
      "loss.mean()=tensor(0.9037, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18297553062438965\n",
      "loss.mean()=tensor(0.8977, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19317682087421417\n",
      "loss.mean()=tensor(0.9030, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1844586730003357\n",
      "loss.mean()=tensor(0.8709, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1821640431880951\n",
      "loss.mean()=tensor(0.8563, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17936494946479797\n",
      "loss.mean()=tensor(0.8872, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8872169852256775\n",
      "ep 84\n",
      "---------------------\n",
      "outputs.mean().item()=0.1816319078207016\n",
      "loss.mean()=tensor(0.8665, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.16882926225662231\n",
      "loss.mean()=tensor(0.9060, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1734803467988968\n",
      "loss.mean()=tensor(0.8416, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1878689080476761\n",
      "loss.mean()=tensor(0.8634, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19206854701042175\n",
      "loss.mean()=tensor(0.9475, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18824878334999084\n",
      "loss.mean()=tensor(0.8482, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18156087398529053\n",
      "loss.mean()=tensor(0.8976, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18532131612300873\n",
      "loss.mean()=tensor(0.8921, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19558434188365936\n",
      "loss.mean()=tensor(0.8972, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18681569397449493\n",
      "loss.mean()=tensor(0.8650, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18445704877376556\n",
      "loss.mean()=tensor(0.8501, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1816878765821457\n",
      "loss.mean()=tensor(0.8811, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8810967206954956\n",
      "ep 85\n",
      "---------------------\n",
      "outputs.mean().item()=0.18392334878444672\n",
      "loss.mean()=tensor(0.8608, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17107108235359192\n",
      "loss.mean()=tensor(0.9008, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17578831315040588\n",
      "loss.mean()=tensor(0.8356, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19023321568965912\n",
      "loss.mean()=tensor(0.8569, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1945141851902008\n",
      "loss.mean()=tensor(0.9419, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1905936747789383\n",
      "loss.mean()=tensor(0.8428, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18391087651252747\n",
      "loss.mean()=tensor(0.8916, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18763557076454163\n",
      "loss.mean()=tensor(0.8868, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19792333245277405\n",
      "loss.mean()=tensor(0.8917, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1890954077243805\n",
      "loss.mean()=tensor(0.8593, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18669642508029938\n",
      "loss.mean()=tensor(0.8441, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18396012485027313\n",
      "loss.mean()=tensor(0.8752, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8751909732818604\n",
      "ep 86\n",
      "---------------------\n",
      "outputs.mean().item()=0.18620136380195618\n",
      "loss.mean()=tensor(0.8552, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17333540320396423\n",
      "loss.mean()=tensor(0.8957, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17810086905956268\n",
      "loss.mean()=tensor(0.8297, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19260069727897644\n",
      "loss.mean()=tensor(0.8506, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19693492352962494\n",
      "loss.mean()=tensor(0.9365, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19291067123413086\n",
      "loss.mean()=tensor(0.8375, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18624773621559143\n",
      "loss.mean()=tensor(0.8859, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18990947306156158\n",
      "loss.mean()=tensor(0.8816, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20023667812347412\n",
      "loss.mean()=tensor(0.8864, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19139033555984497\n",
      "loss.mean()=tensor(0.8538, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.188945010304451\n",
      "loss.mean()=tensor(0.8382, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1862313449382782\n",
      "loss.mean()=tensor(0.8695, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8694925904273987\n",
      "ep 87\n",
      "---------------------\n",
      "outputs.mean().item()=0.18843811750411987\n",
      "loss.mean()=tensor(0.8499, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17552629113197327\n",
      "loss.mean()=tensor(0.8908, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18031230568885803\n",
      "loss.mean()=tensor(0.8241, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1948316991329193\n",
      "loss.mean()=tensor(0.8445, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19922761619091034\n",
      "loss.mean()=tensor(0.9312, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19510366022586823\n",
      "loss.mean()=tensor(0.8325, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1884448528289795\n",
      "loss.mean()=tensor(0.8804, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19203510880470276\n",
      "loss.mean()=tensor(0.8767, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20238757133483887\n",
      "loss.mean()=tensor(0.8812, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19354817271232605\n",
      "loss.mean()=tensor(0.8484, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1910540610551834\n",
      "loss.mean()=tensor(0.8326, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18836826086044312\n",
      "loss.mean()=tensor(0.8640, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8639664053916931\n",
      "ep 88\n",
      "---------------------\n",
      "outputs.mean().item()=0.19054044783115387\n",
      "loss.mean()=tensor(0.8447, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.17760628461837769\n",
      "loss.mean()=tensor(0.8861, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18240801990032196\n",
      "loss.mean()=tensor(0.8187, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1969599574804306\n",
      "loss.mean()=tensor(0.8386, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20143170654773712\n",
      "loss.mean()=tensor(0.9261, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1972239911556244\n",
      "loss.mean()=tensor(0.8276, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19063784182071686\n",
      "loss.mean()=tensor(0.8750, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19412542879581451\n",
      "loss.mean()=tensor(0.8719, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2045307606458664\n",
      "loss.mean()=tensor(0.8762, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19568096101284027\n",
      "loss.mean()=tensor(0.8433, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1931353509426117\n",
      "loss.mean()=tensor(0.8271, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.190481036901474\n",
      "loss.mean()=tensor(0.8586, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.858599066734314\n",
      "ep 89\n",
      "---------------------\n",
      "outputs.mean().item()=0.19264447689056396\n",
      "loss.mean()=tensor(0.8397, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1796918362379074\n",
      "loss.mean()=tensor(0.8815, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18454691767692566\n",
      "loss.mean()=tensor(0.8134, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19913606345653534\n",
      "loss.mean()=tensor(0.8329, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20370373129844666\n",
      "loss.mean()=tensor(0.9211, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19940900802612305\n",
      "loss.mean()=tensor(0.8230, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19285327196121216\n",
      "loss.mean()=tensor(0.8699, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19627103209495544\n",
      "loss.mean()=tensor(0.8673, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20673485100269318\n",
      "loss.mean()=tensor(0.8713, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1978704035282135\n",
      "loss.mean()=tensor(0.8383, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19525820016860962\n",
      "loss.mean()=tensor(0.8218, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19261182844638824\n",
      "loss.mean()=tensor(0.8534, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8533920049667358\n",
      "ep 90\n",
      "---------------------\n",
      "outputs.mean().item()=0.19475087523460388\n",
      "loss.mean()=tensor(0.8348, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18177910149097443\n",
      "loss.mean()=tensor(0.8771, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1867084503173828\n",
      "loss.mean()=tensor(0.8083, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20133523643016815\n",
      "loss.mean()=tensor(0.8273, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2059561014175415\n",
      "loss.mean()=tensor(0.9163, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20155353844165802\n",
      "loss.mean()=tensor(0.8184, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1950111836194992\n",
      "loss.mean()=tensor(0.8649, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1983739584684372\n",
      "loss.mean()=tensor(0.8629, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20888063311576843\n",
      "loss.mean()=tensor(0.8666, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2000090777873993\n",
      "loss.mean()=tensor(0.8335, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1973412036895752\n",
      "loss.mean()=tensor(0.8167, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19474942982196808\n",
      "loss.mean()=tensor(0.8483, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8483405113220215\n",
      "ep 91\n",
      "---------------------\n",
      "outputs.mean().item()=0.19686420261859894\n",
      "loss.mean()=tensor(0.8301, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18384109437465668\n",
      "loss.mean()=tensor(0.8729, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18884725868701935\n",
      "loss.mean()=tensor(0.8033, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20348414778709412\n",
      "loss.mean()=tensor(0.8220, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20816747844219208\n",
      "loss.mean()=tensor(0.9115, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2036595642566681\n",
      "loss.mean()=tensor(0.8140, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19713735580444336\n",
      "loss.mean()=tensor(0.8600, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20043936371803284\n",
      "loss.mean()=tensor(0.8586, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21099552512168884\n",
      "loss.mean()=tensor(0.8620, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2021045684814453\n",
      "loss.mean()=tensor(0.8288, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19940893352031708\n",
      "loss.mean()=tensor(0.8117, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19686996936798096\n",
      "loss.mean()=tensor(0.8434, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8434407711029053\n",
      "ep 92\n",
      "---------------------\n",
      "outputs.mean().item()=0.19893142580986023\n",
      "loss.mean()=tensor(0.8255, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1858723759651184\n",
      "loss.mean()=tensor(0.8687, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19093450903892517\n",
      "loss.mean()=tensor(0.7985, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20556774735450745\n",
      "loss.mean()=tensor(0.8168, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2103271186351776\n",
      "loss.mean()=tensor(0.9069, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20575018227100372\n",
      "loss.mean()=tensor(0.8098, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19927243888378143\n",
      "loss.mean()=tensor(0.8553, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20252199470996857\n",
      "loss.mean()=tensor(0.8545, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21311894059181213\n",
      "loss.mean()=tensor(0.8575, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20420698821544647\n",
      "loss.mean()=tensor(0.8243, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2014545500278473\n",
      "loss.mean()=tensor(0.8069, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19895640015602112\n",
      "loss.mean()=tensor(0.8387, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.838678777217865\n",
      "ep 93\n",
      "---------------------\n",
      "outputs.mean().item()=0.20096838474273682\n",
      "loss.mean()=tensor(0.8211, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18788139522075653\n",
      "loss.mean()=tensor(0.8647, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1930018663406372\n",
      "loss.mean()=tensor(0.7939, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20765690505504608\n",
      "loss.mean()=tensor(0.8117, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2124977558851242\n",
      "loss.mean()=tensor(0.9025, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20786115527153015\n",
      "loss.mean()=tensor(0.8057, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2014201432466507\n",
      "loss.mean()=tensor(0.8508, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20458674430847168\n",
      "loss.mean()=tensor(0.8505, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21518388390541077\n",
      "loss.mean()=tensor(0.8532, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20626303553581238\n",
      "loss.mean()=tensor(0.8199, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20345577597618103\n",
      "loss.mean()=tensor(0.8022, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20098912715911865\n",
      "loss.mean()=tensor(0.8341, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8340505361557007\n",
      "ep 94\n",
      "---------------------\n",
      "outputs.mean().item()=0.20301131904125214\n",
      "loss.mean()=tensor(0.8167, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.18987663090229034\n",
      "loss.mean()=tensor(0.8608, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19505956768989563\n",
      "loss.mean()=tensor(0.7894, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20971941947937012\n",
      "loss.mean()=tensor(0.8068, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21460624039173126\n",
      "loss.mean()=tensor(0.8981, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20991654694080353\n",
      "loss.mean()=tensor(0.8018, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2035117894411087\n",
      "loss.mean()=tensor(0.8464, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20657043159008026\n",
      "loss.mean()=tensor(0.8467, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21720819175243378\n",
      "loss.mean()=tensor(0.8490, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20824627578258514\n",
      "loss.mean()=tensor(0.8157, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20538464188575745\n",
      "loss.mean()=tensor(0.7977, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20297999680042267\n",
      "loss.mean()=tensor(0.8295, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8295419216156006\n",
      "ep 95\n",
      "---------------------\n",
      "outputs.mean().item()=0.20499154925346375\n",
      "loss.mean()=tensor(0.8125, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19180849194526672\n",
      "loss.mean()=tensor(0.8570, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.1970285177230835\n",
      "loss.mean()=tensor(0.7850, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21170854568481445\n",
      "loss.mean()=tensor(0.8021, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.216658353805542\n",
      "loss.mean()=tensor(0.8938, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2119128406047821\n",
      "loss.mean()=tensor(0.7980, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20555195212364197\n",
      "loss.mean()=tensor(0.8421, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20854301750659943\n",
      "loss.mean()=tensor(0.8430, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21923251450061798\n",
      "loss.mean()=tensor(0.8450, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21025829017162323\n",
      "loss.mean()=tensor(0.8115, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20734170079231262\n",
      "loss.mean()=tensor(0.7932, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20498639345169067\n",
      "loss.mean()=tensor(0.8252, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.825156569480896\n",
      "ep 96\n",
      "---------------------\n",
      "outputs.mean().item()=0.20699207484722137\n",
      "loss.mean()=tensor(0.8085, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19378121197223663\n",
      "loss.mean()=tensor(0.8534, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19908085465431213\n",
      "loss.mean()=tensor(0.7807, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21377190947532654\n",
      "loss.mean()=tensor(0.7975, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21881265938282013\n",
      "loss.mean()=tensor(0.8897, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21398501098155975\n",
      "loss.mean()=tensor(0.7943, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20761457085609436\n",
      "loss.mean()=tensor(0.8380, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21055594086647034\n",
      "loss.mean()=tensor(0.8394, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22126397490501404\n",
      "loss.mean()=tensor(0.8410, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21228928864002228\n",
      "loss.mean()=tensor(0.8076, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20931582152843475\n",
      "loss.mean()=tensor(0.7889, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20700451731681824\n",
      "loss.mean()=tensor(0.8209, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.820891261100769\n",
      "ep 97\n",
      "---------------------\n",
      "outputs.mean().item()=0.20900511741638184\n",
      "loss.mean()=tensor(0.8045, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19579532742500305\n",
      "loss.mean()=tensor(0.8498, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2011413872241974\n",
      "loss.mean()=tensor(0.7766, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21584105491638184\n",
      "loss.mean()=tensor(0.7930, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22094735503196716\n",
      "loss.mean()=tensor(0.8856, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21605291962623596\n",
      "loss.mean()=tensor(0.7907, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2097148895263672\n",
      "loss.mean()=tensor(0.8339, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21255633234977722\n",
      "loss.mean()=tensor(0.8359, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22331185638904572\n",
      "loss.mean()=tensor(0.8371, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21433702111244202\n",
      "loss.mean()=tensor(0.8037, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21130821108818054\n",
      "loss.mean()=tensor(0.7848, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20901457965373993\n",
      "loss.mean()=tensor(0.8167, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8167400360107422\n",
      "ep 98\n",
      "---------------------\n",
      "outputs.mean().item()=0.21100583672523499\n",
      "loss.mean()=tensor(0.8006, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19776757061481476\n",
      "loss.mean()=tensor(0.8463, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20315685868263245\n",
      "loss.mean()=tensor(0.7726, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21786360442638397\n",
      "loss.mean()=tensor(0.7887, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22303462028503418\n",
      "loss.mean()=tensor(0.8816, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2180785834789276\n",
      "loss.mean()=tensor(0.7872, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21176262199878693\n",
      "loss.mean()=tensor(0.8300, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21453595161437988\n",
      "loss.mean()=tensor(0.8326, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22532658278942108\n",
      "loss.mean()=tensor(0.8334, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2163144052028656\n",
      "loss.mean()=tensor(0.7999, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.213222935795784\n",
      "loss.mean()=tensor(0.7807, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21096915006637573\n",
      "loss.mean()=tensor(0.8127, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8126970529556274\n",
      "ep 99\n",
      "---------------------\n",
      "outputs.mean().item()=0.21294760704040527\n",
      "loss.mean()=tensor(0.7968, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.19971175491809845\n",
      "loss.mean()=tensor(0.8430, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.20513872802257538\n",
      "loss.mean()=tensor(0.7686, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21984350681304932\n",
      "loss.mean()=tensor(0.7845, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22507520020008087\n",
      "loss.mean()=tensor(0.8777, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22004303336143494\n",
      "loss.mean()=tensor(0.7839, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.213768869638443\n",
      "loss.mean()=tensor(0.8263, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2165130078792572\n",
      "loss.mean()=tensor(0.8294, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.22730977833271027\n",
      "loss.mean()=tensor(0.8297, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.21825993061065674\n",
      "loss.mean()=tensor(0.7963, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2151072472333908\n",
      "loss.mean()=tensor(0.7768, grad_fn=<MeanBackward0>)\n",
      "outputs.mean().item()=0.2128896564245224\n",
      "loss.mean()=tensor(0.8088, grad_fn=<MeanBackward0>)\n",
      "(loss.item())=0.8087553977966309\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear1 = torch.nn.Linear(784, 784)\n",
    "        self.linear2 = torch.nn.Linear(784, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 128)\n",
    "        self.linear4 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'{x[0].mean()=}')\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear4(x)\n",
    "        # print(f'{x[0].mean()=}')\n",
    "        # x = self.activation(x)\n",
    "        return x\n",
    "        \n",
    "tinymodel = TinyModel()\n",
    "init_weight1 = deepcopy(tinymodel.linear1.weight.detach())\n",
    "init_bias1 = deepcopy(tinymodel.linear1.bias.detach())\n",
    "\n",
    "init_weight2 = deepcopy(tinymodel.linear2.weight.detach())\n",
    "init_bias2 = deepcopy(tinymodel.linear2.bias.detach())\n",
    "\n",
    "\n",
    "init_weight3 = deepcopy(tinymodel.linear3.weight.detach())\n",
    "init_bias3 = deepcopy(tinymodel.linear3.bias.detach())\n",
    "\n",
    "\n",
    "init_weight4 = deepcopy(tinymodel.linear4.weight.detach())\n",
    "init_bias4 = deepcopy(tinymodel.linear4.bias.detach())\n",
    "\n",
    "optimizer = torch.optim.SGD(tinymodel.parameters(), lr=0.01, weight_decay=0.0007)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "l = []\n",
    "means = []\n",
    "\n",
    "def train(l, m):\n",
    "    for ep in range(100):\n",
    "        print('ep', ep)\n",
    "        print('---------------------')\n",
    "        for x_batch, y_batch in get_batches(X_test_small, y_test_small, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            # Make predictions for this batch\n",
    "            outputs = tinymodel(x_batch)\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, y_batch.long())\n",
    "            print(f'{outputs.mean().item()=}')\n",
    "            m.append(outputs.mean().item())\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            #print('grad', tinymodel.linear1.weight.grad[0].mean())\n",
    "            # Adjust learning weights\n",
    "            print(f'{loss.mean()=}')\n",
    "            optimizer.step()\n",
    "        loss = loss_fn(outputs, y_batch.long())\n",
    "        l.append(loss.item())\n",
    "        print(f'{(loss.item())=}')\n",
    "train(l, means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:49:53.190179Z",
     "start_time": "2024-10-28T08:49:53.185036Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, num_ep=400, use_old_weight=True, init_lr=0.025, decay_sched=None):\n",
    "    \n",
    "    loss_tr = []\n",
    "    loss_val = []\n",
    "\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "\n",
    "    for ep in range(num_ep):\n",
    "        print('ep', ep)\n",
    "        print('---------------------')\n",
    "        for i, batch in enumerate(get_batches(X_test_small, y_test_small, batch_size)):\n",
    "            x_batch, y_batch = batch\n",
    "            model.fit(x_batch, y_batch, use_old_weight)\n",
    "            \n",
    "        loss_tr.append(model.get_loss())\n",
    "        print(loss_tr[-1].item())\n",
    "        # loss_val.append(model.get_val_loss(X_test_small, y_test_small))\n",
    "        # acc_tr.append(model.score(X_train_small, y_train_small))\n",
    "        # acc_val.append(model.score(X_test_small, y_test_small))\n",
    "        # print('ep: ', ep, 'Training accuracy: ', acc_tr[-1], 'Test accuracy:', acc_val[-1], 'Train loss:', loss_tr[-1], 'Val loss:', loss_val[-1],)\n",
    "        \n",
    "        if (decay_sched != None) and (ep in decay_sched):\n",
    "            alpha = decay_sched[ep]\n",
    "            model.change_alpha(alpha)\n",
    "            \n",
    "    return loss_tr, loss_val, acc_tr, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T08:50:01.721923Z",
     "start_time": "2024-10-28T08:50:00.196423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0\n",
      "---------------------\n",
      "classical output 0.003540846286341548\n",
      "classical output 0.0036507253535091877\n",
      "classical output 0.0035804682411253452\n",
      "classical output 0.0036925661843270063\n",
      "classical output 0.003603277262300253\n",
      "classical output 0.0038026180118322372\n",
      "classical output 0.003827861975878477\n",
      "classical output 0.003803583327680826\n",
      "classical output 0.0036657308228313923\n",
      "classical output 0.003767918096855283\n",
      "classical output 0.0037139025516808033\n",
      "classical output 0.0037181975785642862\n",
      "2.303987979888916\n",
      "ep 1\n",
      "---------------------\n",
      "classical output 0.0037004705518484116\n",
      "classical output 0.003797060577198863\n",
      "classical output 0.0037432133685797453\n",
      "classical output 0.0038630228955298662\n",
      "classical output 0.003759498242288828\n",
      "classical output 0.003966012038290501\n",
      "classical output 0.003994374070316553\n",
      "classical output 0.003962283488363028\n",
      "classical output 0.0038310338277369738\n",
      "classical output 0.003929511643946171\n",
      "classical output 0.0038844365626573563\n",
      "classical output 0.003876008093357086\n",
      "2.3007097244262695\n",
      "ep 2\n",
      "---------------------\n",
      "classical output 0.00386373745277524\n",
      "classical output 0.003948750905692577\n",
      "classical output 0.0039197844453155994\n",
      "classical output 0.00405073631554842\n",
      "classical output 0.003932742401957512\n",
      "classical output 0.004156008828431368\n",
      "classical output 0.004186974838376045\n",
      "classical output 0.004157216288149357\n",
      "classical output 0.004044384229928255\n",
      "classical output 0.0041203428991138935\n",
      "classical output 0.004093021620064974\n",
      "classical output 0.0040735206566751\n",
      "2.2973837852478027\n",
      "ep 3\n",
      "---------------------\n",
      "classical output 0.004069880582392216\n",
      "classical output 0.0041469731368124485\n",
      "classical output 0.004146172199398279\n",
      "classical output 0.004284282214939594\n",
      "classical output 0.004156448878347874\n",
      "classical output 0.0043939040042459965\n",
      "classical output 0.004415223840624094\n",
      "classical output 0.004394575022161007\n",
      "classical output 0.004301026463508606\n",
      "classical output 0.004356222692877054\n",
      "classical output 0.004342643544077873\n",
      "classical output 0.004308449104428291\n",
      "2.29406476020813\n",
      "ep 4\n",
      "---------------------\n",
      "classical output 0.004316722042858601\n",
      "classical output 0.004382256884127855\n",
      "classical output 0.00440481211990118\n",
      "classical output 0.0045448122546076775\n",
      "classical output 0.004413538612425327\n",
      "classical output 0.004653166048228741\n",
      "classical output 0.004666557069867849\n",
      "classical output 0.00465696444734931\n",
      "classical output 0.004579349420964718\n",
      "classical output 0.004620191641151905\n",
      "classical output 0.004615417681634426\n",
      "classical output 0.004569673910737038\n",
      "2.290731191635132\n",
      "ep 5\n",
      "---------------------\n",
      "classical output 0.004583575762808323\n",
      "classical output 0.004641852341592312\n",
      "classical output 0.004681968130171299\n",
      "classical output 0.004826823715120554\n",
      "classical output 0.004697081632912159\n",
      "classical output 0.004938638303428888\n",
      "classical output 0.004945148713886738\n",
      "classical output 0.004948821850121021\n",
      "classical output 0.004883160348981619\n",
      "classical output 0.00491365697234869\n",
      "classical output 0.004913817159831524\n",
      "classical output 0.004860320128500462\n",
      "2.2873361110687256\n",
      "ep 6\n",
      "---------------------\n",
      "classical output 0.0048852055333554745\n",
      "classical output 0.004937982652336359\n",
      "classical output 0.0049933395348489285\n",
      "classical output 0.005138879641890526\n",
      "classical output 0.005019287578761578\n",
      "classical output 0.005263508763164282\n",
      "classical output 0.00526857515797019\n",
      "classical output 0.005281616002321243\n",
      "classical output 0.005232446361333132\n",
      "classical output 0.00525705236941576\n",
      "classical output 0.0052634174935519695\n",
      "classical output 0.0052080582827329636\n",
      "2.2838246822357178\n",
      "ep 7\n",
      "---------------------\n",
      "classical output 0.005246034823358059\n",
      "classical output 0.005294275004416704\n",
      "classical output 0.005366723984479904\n",
      "classical output 0.005518378224223852\n",
      "classical output 0.005412568803876638\n",
      "classical output 0.005652081221342087\n",
      "classical output 0.005664770491421223\n",
      "classical output 0.005686730146408081\n",
      "classical output 0.005651154555380344\n",
      "classical output 0.005670602433383465\n",
      "classical output 0.005680044647306204\n",
      "classical output 0.005624639336019754\n",
      "2.2801103591918945\n",
      "ep 8\n",
      "---------------------\n",
      "classical output 0.005673672072589397\n",
      "classical output 0.005719196051359177\n",
      "classical output 0.005809427239000797\n",
      "classical output 0.005972596816718578\n",
      "classical output 0.005876363255083561\n",
      "classical output 0.006108588073402643\n",
      "classical output 0.006128285080194473\n",
      "classical output 0.006154830567538738\n",
      "classical output 0.00614001601934433\n",
      "classical output 0.006156472954899073\n",
      "classical output 0.006168084219098091\n",
      "classical output 0.006110732443630695\n",
      "2.2761778831481934\n",
      "ep 9\n",
      "---------------------\n",
      "classical output 0.006170365028083324\n",
      "classical output 0.006214693188667297\n",
      "classical output 0.006324950605630875\n",
      "classical output 0.006498901639133692\n",
      "classical output 0.0064126430079340935\n",
      "classical output 0.006636659614741802\n",
      "classical output 0.006662464700639248\n",
      "classical output 0.006693939212709665\n",
      "classical output 0.006699816789478064\n",
      "classical output 0.006707075983285904\n",
      "classical output 0.006720358971506357\n",
      "classical output 0.00666262861341238\n",
      "2.2720088958740234\n",
      "ep 10\n",
      "---------------------\n",
      "classical output 0.0067307958379387856\n",
      "classical output 0.006767557002604008\n",
      "classical output 0.006898538675159216\n",
      "classical output 0.007084654178470373\n",
      "classical output 0.0070100994780659676\n",
      "classical output 0.007226578891277313\n",
      "classical output 0.007259822450578213\n",
      "classical output 0.007291724439710379\n",
      "classical output 0.007320425473153591\n",
      "classical output 0.007314559072256088\n",
      "classical output 0.007330679800361395\n",
      "classical output 0.007272692397236824\n",
      "2.267615795135498\n",
      "ep 11\n",
      "---------------------\n",
      "classical output 0.00735058169811964\n",
      "classical output 0.007379274815320969\n",
      "classical output 0.007527491543442011\n",
      "classical output 0.007727146148681641\n",
      "classical output 0.007656899280846119\n",
      "classical output 0.007863521575927734\n",
      "classical output 0.007900038734078407\n",
      "classical output 0.00793440081179142\n",
      "classical output 0.007985347881913185\n",
      "classical output 0.00796265434473753\n",
      "classical output 0.007978365756571293\n",
      "classical output 0.007923666387796402\n",
      "2.2629923820495605\n",
      "ep 12\n",
      "---------------------\n",
      "classical output 0.008010748773813248\n",
      "classical output 0.00802499521523714\n",
      "classical output 0.008194463327527046\n",
      "classical output 0.008411755785346031\n",
      "classical output 0.008346321992576122\n",
      "classical output 0.008536351844668388\n",
      "classical output 0.008574744686484337\n",
      "classical output 0.008612779900431633\n",
      "classical output 0.008689671754837036\n",
      "classical output 0.008650535717606544\n",
      "classical output 0.008662122301757336\n",
      "classical output 0.008612517267465591\n",
      "2.2581064701080322\n",
      "ep 13\n",
      "---------------------\n",
      "classical output 0.008701344951987267\n",
      "classical output 0.008700395002961159\n",
      "classical output 0.008894426748156548\n",
      "classical output 0.009129339829087257\n",
      "classical output 0.00906488299369812\n",
      "classical output 0.00923724565654993\n",
      "classical output 0.009279323741793633\n",
      "classical output 0.00931779108941555\n",
      "classical output 0.009423131123185158\n",
      "classical output 0.009366252459585667\n",
      "classical output 0.009369170293211937\n",
      "classical output 0.009323857724666595\n",
      "2.252917528152466\n",
      "ep 14\n",
      "---------------------\n",
      "classical output 0.009419086389243603\n",
      "classical output 0.009400567971169949\n",
      "classical output 0.009616583585739136\n",
      "classical output 0.009870327077805996\n",
      "classical output 0.009806735441088676\n",
      "classical output 0.00995901133865118\n",
      "classical output 0.010011976584792137\n",
      "classical output 0.01004597544670105\n",
      "classical output 0.010182099416851997\n",
      "classical output 0.010105399414896965\n",
      "classical output 0.010101531632244587\n",
      "classical output 0.010063818655908108\n",
      "2.2473695278167725\n",
      "ep 15\n",
      "---------------------\n",
      "classical output 0.010164329782128334\n",
      "classical output 0.010128476656973362\n",
      "classical output 0.010372447781264782\n",
      "classical output 0.010649253614246845\n",
      "classical output 0.010590036399662495\n",
      "classical output 0.01071744691580534\n",
      "classical output 0.010782571509480476\n",
      "classical output 0.01081656664609909\n",
      "classical output 0.010983781889081001\n",
      "classical output 0.010885408148169518\n",
      "classical output 0.01087771076709032\n",
      "classical output 0.010847296565771103\n",
      "2.2414238452911377\n",
      "ep 16\n",
      "---------------------\n",
      "classical output 0.010952040553092957\n",
      "classical output 0.010895540937781334\n",
      "classical output 0.011174149811267853\n",
      "classical output 0.011473451741039753\n",
      "classical output 0.011419776827096939\n",
      "classical output 0.01152135245501995\n",
      "classical output 0.011598126031458378\n",
      "classical output 0.011629588901996613\n",
      "classical output 0.011827951297163963\n",
      "classical output 0.011707508936524391\n",
      "classical output 0.01169606763869524\n",
      "classical output 0.011673013679683208\n",
      "2.2350199222564697\n",
      "ep 17\n",
      "---------------------\n",
      "classical output 0.011781147681176662\n",
      "classical output 0.011702049523591995\n",
      "classical output 0.012014942243695259\n",
      "classical output 0.012336218729615211\n",
      "classical output 0.012290535494685173\n",
      "classical output 0.012367498129606247\n",
      "classical output 0.012454638257622719\n",
      "classical output 0.012482983060181141\n",
      "classical output 0.012714624404907227\n",
      "classical output 0.012569782324135303\n",
      "classical output 0.012558442540466785\n",
      "classical output 0.01254234742373228\n",
      "2.228116273880005\n",
      "ep 18\n",
      "---------------------\n",
      "classical output 0.012659996747970581\n",
      "classical output 0.012556935660541058\n",
      "classical output 0.01290818490087986\n",
      "classical output 0.013253492303192616\n",
      "classical output 0.013216224499046803\n",
      "classical output 0.013267564587295055\n",
      "classical output 0.013367950916290283\n",
      "classical output 0.013393191620707512\n",
      "classical output 0.013664436526596546\n",
      "classical output 0.013489583507180214\n",
      "classical output 0.01347455196082592\n",
      "classical output 0.013464521616697311\n",
      "2.2206177711486816\n",
      "ep 19\n",
      "---------------------\n",
      "classical output 0.013592729344964027\n",
      "classical output 0.013463333249092102\n",
      "classical output 0.01385404635220766\n",
      "classical output 0.014229686930775642\n",
      "classical output 0.014201348647475243\n",
      "classical output 0.014228549785912037\n",
      "classical output 0.014340931549668312\n",
      "classical output 0.014364729635417461\n",
      "classical output 0.014681100845336914\n",
      "classical output 0.014480838552117348\n",
      "classical output 0.014456445351243019\n",
      "classical output 0.014456136152148247\n",
      "2.2124481201171875\n",
      "ep 20\n",
      "---------------------\n",
      "classical output 0.014599126763641834\n",
      "classical output 0.014446886256337166\n",
      "classical output 0.014875387772917747\n",
      "classical output 0.015284771099686623\n",
      "classical output 0.015270642936229706\n",
      "classical output 0.015273233875632286\n",
      "classical output 0.015397211536765099\n",
      "classical output 0.015421261079609394\n",
      "classical output 0.0157840047031641\n",
      "classical output 0.015559740364551544\n",
      "classical output 0.015523673966526985\n",
      "classical output 0.015530980192124844\n",
      "2.2035176753997803\n",
      "ep 21\n",
      "---------------------\n",
      "classical output 0.01569000631570816\n",
      "classical output 0.01551269181072712\n",
      "classical output 0.015977542847394943\n",
      "classical output 0.016430508345365524\n",
      "classical output 0.01643006131052971\n",
      "classical output 0.01641005650162697\n",
      "classical output 0.016547895967960358\n",
      "classical output 0.0165739506483078\n",
      "classical output 0.016989702358841896\n",
      "classical output 0.016738656908273697\n",
      "classical output 0.01669103465974331\n",
      "classical output 0.016711823642253876\n",
      "2.193746566772461\n",
      "ep 22\n",
      "---------------------\n",
      "classical output 0.016892068088054657\n",
      "classical output 0.016684774309396744\n",
      "classical output 0.017191465944051743\n",
      "classical output 0.01768599823117256\n",
      "classical output 0.017706837505102158\n",
      "classical output 0.017660042271018028\n",
      "classical output 0.017810774967074394\n",
      "classical output 0.01784994825720787\n",
      "classical output 0.018318384885787964\n",
      "classical output 0.018042288720607758\n",
      "classical output 0.01798500493168831\n",
      "classical output 0.01802152208983898\n",
      "2.1830310821533203\n",
      "ep 23\n",
      "---------------------\n",
      "classical output 0.01822497509419918\n",
      "classical output 0.017980270087718964\n",
      "classical output 0.01854320429265499\n",
      "classical output 0.019080182537436485\n",
      "classical output 0.019124997779726982\n",
      "classical output 0.01905464008450508\n",
      "classical output 0.019221367314457893\n",
      "classical output 0.019272685050964355\n",
      "classical output 0.019795197993516922\n",
      "classical output 0.01949278637766838\n",
      "classical output 0.01942208781838417\n",
      "classical output 0.01947649009525776\n",
      "2.1712417602539062\n",
      "ep 24\n",
      "---------------------\n",
      "classical output 0.01970123127102852\n",
      "classical output 0.019415615126490593\n",
      "classical output 0.020047951489686966\n",
      "classical output 0.020639853551983833\n",
      "classical output 0.020706817507743835\n",
      "classical output 0.020601309835910797\n",
      "classical output 0.02079557627439499\n",
      "classical output 0.02085663005709648\n",
      "classical output 0.021441806107759476\n",
      "classical output 0.021108034998178482\n",
      "classical output 0.021025273948907852\n",
      "classical output 0.021089665591716766\n",
      "2.158273696899414\n",
      "ep 25\n",
      "---------------------\n",
      "classical output 0.021345429122447968\n",
      "classical output 0.02101922407746315\n",
      "classical output 0.02171478234231472\n",
      "classical output 0.022365596145391464\n",
      "classical output 0.022452374920248985\n",
      "classical output 0.02231481298804283\n",
      "classical output 0.0225395355373621\n",
      "classical output 0.022614870220422745\n",
      "classical output 0.023268062621355057\n",
      "classical output 0.022896533831954002\n",
      "classical output 0.022795256227254868\n",
      "classical output 0.022871974855661392\n",
      "2.144010066986084\n",
      "ep 26\n",
      "---------------------\n",
      "classical output 0.02315186895430088\n",
      "classical output 0.02276906743645668\n",
      "classical output 0.02353212796151638\n",
      "classical output 0.024251561611890793\n",
      "classical output 0.02436051331460476\n",
      "classical output 0.024189690127968788\n",
      "classical output 0.02443096600472927\n",
      "classical output 0.024518396705389023\n",
      "classical output 0.025237809866666794\n",
      "classical output 0.024813733994960785\n",
      "classical output 0.024683255702257156\n",
      "classical output 0.024786073714494705\n",
      "2.1283044815063477\n",
      "ep 27\n",
      "---------------------\n",
      "classical output 0.025097474455833435\n",
      "classical output 0.02465551719069481\n",
      "classical output 0.02549075521528721\n",
      "classical output 0.026288505643606186\n",
      "classical output 0.026419898495078087\n",
      "classical output 0.02620997652411461\n",
      "classical output 0.026469534263014793\n",
      "classical output 0.02657189965248108\n",
      "classical output 0.027377326041460037\n",
      "classical output 0.026903459802269936\n",
      "classical output 0.026743579655885696\n",
      "classical output 0.026877805590629578\n",
      "2.110992193222046\n",
      "ep 28\n",
      "---------------------\n",
      "classical output 0.02722003497183323\n",
      "classical output 0.026702802628278732\n",
      "classical output 0.027620136737823486\n",
      "classical output 0.028492281213402748\n",
      "classical output 0.02864185906946659\n",
      "classical output 0.02839301899075508\n",
      "classical output 0.028666216880083084\n",
      "classical output 0.02878987230360508\n",
      "classical output 0.029680049046874046\n",
      "classical output 0.029147138819098473\n",
      "classical output 0.028959158807992935\n",
      "classical output 0.029116779565811157\n",
      "2.091932535171509\n",
      "ep 29\n",
      "---------------------\n",
      "classical output 0.029492119327187538\n",
      "classical output 0.028887545689940453\n",
      "classical output 0.02989659272134304\n",
      "classical output 0.030862053856253624\n",
      "classical output 0.031049927696585655\n",
      "classical output 0.03075690194964409\n",
      "classical output 0.031046638265252113\n",
      "classical output 0.031197473406791687\n",
      "classical output 0.032178182154893875\n",
      "classical output 0.03157764673233032\n",
      "classical output 0.03135494887828827\n",
      "classical output 0.031540073454380035\n",
      "2.0709524154663086\n",
      "ep 30\n",
      "---------------------\n",
      "classical output 0.03195708990097046\n",
      "classical output 0.031250737607479095\n",
      "classical output 0.0323515459895134\n",
      "classical output 0.033416081219911575\n",
      "classical output 0.03363890200853348\n",
      "classical output 0.03329559415578842\n",
      "classical output 0.03358882665634155\n",
      "classical output 0.0337664857506752\n",
      "classical output 0.0348513200879097\n",
      "classical output 0.03417368605732918\n",
      "classical output 0.03391047194600105\n",
      "classical output 0.0341135673224926\n",
      "2.0478954315185547\n",
      "ep 31\n",
      "---------------------\n",
      "classical output 0.0345718190073967\n",
      "classical output 0.033750273287296295\n",
      "classical output 0.03495175391435623\n",
      "classical output 0.03611792251467705\n",
      "classical output 0.03639078885316849\n",
      "classical output 0.0359870120882988\n",
      "classical output 0.03629045933485031\n",
      "classical output 0.036484040319919586\n",
      "classical output 0.037673644721508026\n",
      "classical output 0.036914631724357605\n",
      "classical output 0.03660465031862259\n",
      "classical output 0.036834970116615295\n",
      "2.022592544555664\n",
      "ep 32\n",
      "---------------------\n",
      "classical output 0.0373309925198555\n",
      "classical output 0.0363878533244133\n",
      "classical output 0.03769983723759651\n",
      "classical output 0.03898239880800247\n",
      "classical output 0.039298634976148605\n",
      "classical output 0.0388178788125515\n",
      "classical output 0.03912549465894699\n",
      "classical output 0.039336271584033966\n",
      "classical output 0.0406511053442955\n",
      "classical output 0.039800889790058136\n",
      "classical output 0.039448607712984085\n",
      "classical output 0.039699047803878784\n",
      "1.9949287176132202\n",
      "ep 33\n",
      "---------------------\n",
      "classical output 0.04024336487054825\n",
      "classical output 0.039167482405900955\n",
      "classical output 0.0405760183930397\n",
      "classical output 0.04198825731873512\n",
      "classical output 0.04234500974416733\n",
      "classical output 0.0417824387550354\n",
      "classical output 0.042101528495550156\n",
      "classical output 0.04233637452125549\n",
      "classical output 0.04378095269203186\n",
      "classical output 0.04282278195023537\n",
      "classical output 0.04242409020662308\n",
      "classical output 0.0426810197532177\n",
      "1.9647940397262573\n",
      "ep 34\n",
      "---------------------\n",
      "classical output 0.04328405484557152\n",
      "classical output 0.0420648455619812\n",
      "classical output 0.04358086735010147\n",
      "classical output 0.04514157772064209\n",
      "classical output 0.04554852098226547\n",
      "classical output 0.04490145295858383\n",
      "classical output 0.045223675668239594\n",
      "classical output 0.04549402743577957\n",
      "classical output 0.04707012325525284\n",
      "classical output 0.04600001499056816\n",
      "classical output 0.04555869102478027\n",
      "classical output 0.045825328677892685\n",
      "1.9321767091751099\n",
      "ep 35\n",
      "---------------------\n",
      "classical output 0.046480726450681686\n",
      "classical output 0.04511607810854912\n",
      "classical output 0.04673155024647713\n",
      "classical output 0.04845958948135376\n",
      "classical output 0.04892348498106003\n",
      "classical output 0.04819643124938011\n",
      "classical output 0.04851653426885605\n",
      "classical output 0.04882500693202019\n",
      "classical output 0.05053282529115677\n",
      "classical output 0.04935305565595627\n",
      "classical output 0.04886956885457039\n",
      "classical output 0.04915280640125275\n",
      "1.8971439599990845\n",
      "ep 36\n",
      "---------------------\n",
      "classical output 0.049861736595630646\n",
      "classical output 0.04834862798452377\n",
      "classical output 0.05007673054933548\n",
      "classical output 0.05197141319513321\n",
      "classical output 0.0525025837123394\n",
      "classical output 0.05168696120381355\n",
      "classical output 0.05201683193445206\n",
      "classical output 0.05236421898007393\n",
      "classical output 0.05421900749206543\n",
      "classical output 0.05291692167520523\n",
      "classical output 0.05238701030611992\n",
      "classical output 0.052686065435409546\n",
      "1.8598945140838623\n",
      "ep 37\n",
      "---------------------\n",
      "classical output 0.05345293879508972\n",
      "classical output 0.05177581310272217\n",
      "classical output 0.05362830311059952\n",
      "classical output 0.055719971656799316\n",
      "classical output 0.05632571130990982\n",
      "classical output 0.05542459338903427\n",
      "classical output 0.055760037153959274\n",
      "classical output 0.05614062398672104\n",
      "classical output 0.05815304070711136\n",
      "classical output 0.05671804025769234\n",
      "classical output 0.05614250898361206\n",
      "classical output 0.05646286532282829\n",
      "1.8207839727401733\n",
      "ep 38\n",
      "---------------------\n",
      "classical output 0.05728545039892197\n",
      "classical output 0.05544012039899826\n",
      "classical output 0.05740156024694443\n",
      "classical output 0.05969588831067085\n",
      "classical output 0.060376912355422974\n",
      "classical output 0.05938934162259102\n",
      "classical output 0.05974326655268669\n",
      "classical output 0.06015272065997124\n",
      "classical output 0.062338776886463165\n",
      "classical output 0.06076846644282341\n",
      "classical output 0.06014305353164673\n",
      "classical output 0.06047872453927994\n",
      "1.7803006172180176\n",
      "ep 39\n",
      "---------------------\n",
      "classical output 0.06137723848223686\n",
      "classical output 0.05935610458254814\n",
      "classical output 0.061450421810150146\n",
      "classical output 0.06395348161458969\n",
      "classical output 0.06471432000398636\n",
      "classical output 0.06361296772956848\n",
      "classical output 0.0639941543340683\n",
      "classical output 0.0644301325082779\n",
      "classical output 0.0668061226606369\n",
      "classical output 0.06510040909051895\n",
      "classical output 0.06440602242946625\n",
      "classical output 0.06474877148866653\n",
      "1.7390416860580444\n",
      "ep 40\n",
      "---------------------\n",
      "classical output 0.0657273381948471\n",
      "classical output 0.06351518630981445\n",
      "classical output 0.06574445217847824\n",
      "classical output 0.06846288591623306\n",
      "classical output 0.06930635869503021\n",
      "classical output 0.06809139251708984\n",
      "classical output 0.06850221008062363\n",
      "classical output 0.06897471845149994\n",
      "classical output 0.0715504065155983\n",
      "classical output 0.069697804749012\n",
      "classical output 0.0689522996544838\n",
      "classical output 0.06929992139339447\n",
      "1.6976039409637451\n",
      "ep 41\n",
      "---------------------\n",
      "classical output 0.07035332918167114\n",
      "classical output 0.06794865429401398\n",
      "classical output 0.07032015919685364\n",
      "classical output 0.07326485216617584\n",
      "classical output 0.07418496161699295\n",
      "classical output 0.0728558897972107\n",
      "classical output 0.07330236583948135\n",
      "classical output 0.07379324734210968\n",
      "classical output 0.07657793909311295\n",
      "classical output 0.07456944137811661\n",
      "classical output 0.07376177608966827\n",
      "classical output 0.07411614060401917\n",
      "1.6565030813217163\n",
      "ep 42\n",
      "---------------------\n",
      "classical output 0.0752427950501442\n",
      "classical output 0.07263655960559845\n",
      "classical output 0.07513505220413208\n",
      "classical output 0.07831204682588577\n",
      "classical output 0.07930267602205276\n",
      "classical output 0.07785449177026749\n",
      "classical output 0.07834430783987045\n",
      "classical output 0.07883554697036743\n",
      "classical output 0.08182506263256073\n",
      "classical output 0.079658642411232\n",
      "classical output 0.07877558469772339\n",
      "classical output 0.07913421839475632\n",
      "1.6161329746246338\n",
      "ep 43\n",
      "---------------------\n",
      "classical output 0.08032002300024033\n",
      "classical output 0.07753585278987885\n",
      "classical output 0.08017627894878387\n",
      "classical output 0.08359013497829437\n",
      "classical output 0.08462880551815033\n",
      "classical output 0.08305613696575165\n",
      "classical output 0.08360081166028976\n",
      "classical output 0.08408939093351364\n",
      "classical output 0.08728402107954025\n",
      "classical output 0.08493656665086746\n",
      "classical output 0.08398747444152832\n",
      "classical output 0.08435385674238205\n",
      "1.5768072605133057\n",
      "ep 44\n",
      "---------------------\n",
      "classical output 0.08559130132198334\n",
      "classical output 0.08262832462787628\n",
      "classical output 0.08540239185094833\n",
      "classical output 0.08903637528419495\n",
      "classical output 0.09014765173196793\n",
      "classical output 0.08842221647500992\n",
      "classical output 0.08903728425502777\n",
      "classical output 0.0895179733633995\n",
      "classical output 0.09292998909950256\n",
      "classical output 0.09041079878807068\n",
      "classical output 0.08936117589473724\n",
      "classical output 0.0897393673658371\n",
      "1.5388239622116089\n",
      "ep 45\n",
      "---------------------\n",
      "classical output 0.0910191535949707\n",
      "classical output 0.08786588907241821\n",
      "classical output 0.09078209102153778\n",
      "classical output 0.09462817013263702\n",
      "classical output 0.09580707550048828\n",
      "classical output 0.09392230212688446\n",
      "classical output 0.09459040313959122\n",
      "classical output 0.09506912529468536\n",
      "classical output 0.09868795424699783\n",
      "classical output 0.09598733484745026\n",
      "classical output 0.09483079612255096\n",
      "classical output 0.09523382782936096\n",
      "1.502341389656067\n",
      "ep 46\n",
      "---------------------\n",
      "classical output 0.09657236188650131\n",
      "classical output 0.0932343527674675\n",
      "classical output 0.09629766643047333\n",
      "classical output 0.10035840421915054\n",
      "classical output 0.10160748660564423\n",
      "classical output 0.09954898804426193\n",
      "classical output 0.10029389709234238\n",
      "classical output 0.10075809061527252\n",
      "classical output 0.10455065965652466\n",
      "classical output 0.10167194902896881\n",
      "classical output 0.10039564222097397\n",
      "classical output 0.10080954432487488\n",
      "1.4674594402313232\n",
      "ep 47\n",
      "---------------------\n",
      "classical output 0.1021856814622879\n",
      "classical output 0.09865991026163101\n",
      "classical output 0.1018480658531189\n",
      "classical output 0.1061224713921547\n",
      "classical output 0.10743017494678497\n",
      "classical output 0.10519769042730331\n",
      "classical output 0.10602221637964249\n",
      "classical output 0.10645736753940582\n",
      "classical output 0.11043564975261688\n",
      "classical output 0.10737667232751846\n",
      "classical output 0.10598517954349518\n",
      "classical output 0.10641209036111832\n",
      "1.4342472553253174\n",
      "ep 48\n",
      "---------------------\n",
      "classical output 0.10782879590988159\n",
      "classical output 0.10410120338201523\n",
      "classical output 0.10742809623479843\n",
      "classical output 0.11190164089202881\n",
      "classical output 0.11328010261058807\n",
      "classical output 0.110869862139225\n",
      "classical output 0.11179041862487793\n",
      "classical output 0.11217381805181503\n",
      "classical output 0.11633596569299698\n",
      "classical output 0.11310295015573502\n",
      "classical output 0.11156760156154633\n",
      "classical output 0.11199526488780975\n",
      "1.402686357498169\n",
      "ep 49\n",
      "---------------------\n",
      "classical output 0.11345691978931427\n",
      "classical output 0.10953857749700546\n",
      "classical output 0.11299826949834824\n",
      "classical output 0.11766453087329865\n",
      "classical output 0.11909377574920654\n",
      "classical output 0.11648212373256683\n",
      "classical output 0.11747662723064423\n",
      "classical output 0.11781581491231918\n",
      "classical output 0.1221473440527916\n",
      "classical output 0.11874624341726303\n",
      "classical output 0.11707460880279541\n",
      "classical output 0.11751458793878555\n",
      "1.3727773427963257\n",
      "ep 50\n",
      "---------------------\n",
      "classical output 0.11901728063821793\n",
      "classical output 0.11490442603826523\n",
      "classical output 0.11850760132074356\n",
      "classical output 0.12338509410619736\n",
      "classical output 0.12487685680389404\n",
      "classical output 0.12205006927251816\n",
      "classical output 0.12312722206115723\n",
      "classical output 0.12342453002929688\n",
      "classical output 0.12792912125587463\n",
      "classical output 0.1243663802742958\n",
      "classical output 0.1225624680519104\n",
      "classical output 0.12298890948295593\n",
      "1.3444583415985107\n",
      "ep 51\n",
      "---------------------\n",
      "classical output 0.12450866401195526\n",
      "classical output 0.12021245807409286\n",
      "classical output 0.1239451915025711\n",
      "classical output 0.12902726233005524\n",
      "classical output 0.13059158623218536\n",
      "classical output 0.1275579035282135\n",
      "classical output 0.12873393297195435\n",
      "classical output 0.12897010147571564\n",
      "classical output 0.13363763689994812\n",
      "classical output 0.12992119789123535\n",
      "classical output 0.12796999514102936\n",
      "classical output 0.12840890884399414\n",
      "1.3176562786102295\n",
      "ep 52\n",
      "---------------------\n",
      "classical output 0.1299707591533661\n",
      "classical output 0.1255062222480774\n",
      "classical output 0.12937131524085999\n",
      "classical output 0.1346561461687088\n",
      "classical output 0.13628701865673065\n",
      "classical output 0.13303439319133759\n",
      "classical output 0.13430757820606232\n",
      "classical output 0.1344732940196991\n",
      "classical output 0.1392969787120819\n",
      "classical output 0.13543812930583954\n",
      "classical output 0.13334046304225922\n",
      "classical output 0.1337972730398178\n",
      "1.2923171520233154\n",
      "ep 53\n",
      "---------------------\n",
      "classical output 0.13539226353168488\n",
      "classical output 0.13076424598693848\n",
      "classical output 0.1347741037607193\n",
      "classical output 0.1402503252029419\n",
      "classical output 0.14194276928901672\n",
      "classical output 0.1384778618812561\n",
      "classical output 0.13985708355903625\n",
      "classical output 0.13995042443275452\n",
      "classical output 0.14493627846240997\n",
      "classical output 0.14093945920467377\n",
      "classical output 0.1386900395154953\n",
      "classical output 0.13915710151195526\n",
      "1.2683653831481934\n",
      "ep 54\n",
      "---------------------\n",
      "classical output 0.140782430768013\n",
      "classical output 0.13599643111228943\n",
      "classical output 0.14013925194740295\n",
      "classical output 0.14579525589942932\n",
      "classical output 0.1475689709186554\n",
      "classical output 0.1438922882080078\n",
      "classical output 0.14539459347724915\n",
      "classical output 0.14541944861412048\n",
      "classical output 0.15056493878364563\n",
      "classical output 0.1464252769947052\n",
      "classical output 0.14402440190315247\n",
      "classical output 0.14450661838054657\n",
      "1.2457157373428345\n",
      "ep 55\n",
      "---------------------\n",
      "classical output 0.14615020155906677\n",
      "classical output 0.1412038505077362\n",
      "classical output 0.1454850733280182\n",
      "classical output 0.15133343636989594\n",
      "classical output 0.1531684696674347\n",
      "classical output 0.14927628636360168\n",
      "classical output 0.15090906620025635\n",
      "classical output 0.15085218846797943\n",
      "classical output 0.15614905953407288\n",
      "classical output 0.15186190605163574\n",
      "classical output 0.14930373430252075\n",
      "classical output 0.14979439973831177\n",
      "1.2242759466171265\n",
      "ep 56\n",
      "---------------------\n",
      "classical output 0.15145964920520782\n",
      "classical output 0.14636585116386414\n",
      "classical output 0.15077224373817444\n",
      "classical output 0.15679822862148285\n",
      "classical output 0.15871068835258484\n",
      "classical output 0.1545906364917755\n",
      "classical output 0.15634410083293915\n",
      "classical output 0.15621426701545715\n",
      "classical output 0.16165152192115784\n",
      "classical output 0.1572446972131729\n",
      "classical output 0.15452340245246887\n",
      "classical output 0.1550324410200119\n",
      "1.2039724588394165\n",
      "ep 57\n",
      "---------------------\n",
      "classical output 0.1567046344280243\n",
      "classical output 0.15146979689598083\n",
      "classical output 0.15601018071174622\n",
      "classical output 0.16220691800117493\n",
      "classical output 0.16419708728790283\n",
      "classical output 0.15984074771404266\n",
      "classical output 0.1617218255996704\n",
      "classical output 0.16150280833244324\n",
      "classical output 0.16709120571613312\n",
      "classical output 0.16255995631217957\n",
      "classical output 0.159684419631958\n",
      "classical output 0.16021336615085602\n",
      "1.1847212314605713\n",
      "ep 58\n",
      "---------------------\n",
      "classical output 0.16189680993556976\n",
      "classical output 0.15651924908161163\n",
      "classical output 0.16118642687797546\n",
      "classical output 0.16753914952278137\n",
      "classical output 0.1696091592311859\n",
      "classical output 0.16501934826374054\n",
      "classical output 0.16703273355960846\n",
      "classical output 0.16670441627502441\n",
      "classical output 0.17242607474327087\n",
      "classical output 0.16777685284614563\n",
      "classical output 0.16474469006061554\n",
      "classical output 0.16530805826187134\n",
      "1.1664315462112427\n",
      "ep 59\n",
      "---------------------\n",
      "classical output 0.16700692474842072\n",
      "classical output 0.16151365637779236\n",
      "classical output 0.16629855334758759\n",
      "classical output 0.17280313372612\n",
      "classical output 0.1749606430530548\n",
      "classical output 0.17014873027801514\n",
      "classical output 0.1722916066646576\n",
      "classical output 0.17184920608997345\n",
      "classical output 0.17767950892448425\n",
      "classical output 0.17291924357414246\n",
      "classical output 0.1697305142879486\n",
      "classical output 0.17032401263713837\n",
      "1.1490379571914673\n",
      "ep 60\n",
      "---------------------\n",
      "classical output 0.1720314472913742\n",
      "classical output 0.16641709208488464\n",
      "classical output 0.17133381962776184\n",
      "classical output 0.1779720038175583\n",
      "classical output 0.1802232563495636\n",
      "classical output 0.17519226670265198\n",
      "classical output 0.17747263610363007\n",
      "classical output 0.1769115924835205\n",
      "classical output 0.18285591900348663\n",
      "classical output 0.17800787091255188\n",
      "classical output 0.1746526062488556\n",
      "classical output 0.17528311908245087\n",
      "1.13247811794281\n",
      "ep 61\n",
      "---------------------\n",
      "classical output 0.1769770085811615\n",
      "classical output 0.17126071453094482\n",
      "classical output 0.17630989849567413\n",
      "classical output 0.18306615948677063\n",
      "classical output 0.18540915846824646\n",
      "classical output 0.18015436828136444\n",
      "classical output 0.18257975578308105\n",
      "classical output 0.18190860748291016\n",
      "classical output 0.18794503808021545\n",
      "classical output 0.1830115020275116\n",
      "classical output 0.17947620153427124\n",
      "classical output 0.18012937903404236\n",
      "1.1166846752166748\n",
      "ep 62\n",
      "---------------------\n",
      "classical output 0.18181149661540985\n",
      "classical output 0.17599043250083923\n",
      "classical output 0.18115553259849548\n",
      "classical output 0.18804951012134552\n",
      "classical output 0.19048002362251282\n",
      "classical output 0.18500661849975586\n",
      "classical output 0.18757131695747375\n",
      "classical output 0.18679344654083252\n",
      "classical output 0.19291934370994568\n",
      "classical output 0.18792298436164856\n",
      "classical output 0.18421567976474762\n",
      "classical output 0.18490569293498993\n",
      "1.1016087532043457\n",
      "ep 63\n",
      "---------------------\n",
      "classical output 0.18658223748207092\n",
      "classical output 0.18065650761127472\n",
      "classical output 0.18593397736549377\n",
      "classical output 0.19294622540473938\n",
      "classical output 0.19546791911125183\n",
      "classical output 0.18977178633213043\n",
      "classical output 0.1924751102924347\n",
      "classical output 0.19159749150276184\n",
      "classical output 0.19782091677188873\n",
      "classical output 0.1927345097064972\n",
      "classical output 0.18887563049793243\n",
      "classical output 0.18960009515285492\n",
      "1.0871928930282593\n",
      "ep 64\n",
      "---------------------\n",
      "classical output 0.19126659631729126\n",
      "classical output 0.18525496125221252\n",
      "classical output 0.19066140055656433\n",
      "classical output 0.19778962433338165\n",
      "classical output 0.20041155815124512\n",
      "classical output 0.19449999928474426\n",
      "classical output 0.19734908640384674\n",
      "classical output 0.1963377594947815\n",
      "classical output 0.20265908539295197\n",
      "classical output 0.19749802350997925\n",
      "classical output 0.19348737597465515\n",
      "classical output 0.19423805177211761\n",
      "1.0733940601348877\n",
      "ep 65\n",
      "---------------------\n",
      "classical output 0.19591493904590607\n",
      "classical output 0.18981975317001343\n",
      "classical output 0.19535402953624725\n",
      "classical output 0.20258530974388123\n",
      "classical output 0.2052961140871048\n",
      "classical output 0.19917340576648712\n",
      "classical output 0.20216350257396698\n",
      "classical output 0.20101633667945862\n",
      "classical output 0.20742730796337128\n",
      "classical output 0.20218753814697266\n",
      "classical output 0.19803068041801453\n",
      "classical output 0.19882862269878387\n",
      "1.0601770877838135\n",
      "ep 66\n",
      "---------------------\n",
      "classical output 0.20047955214977264\n",
      "classical output 0.19430804252624512\n",
      "classical output 0.19993051886558533\n",
      "classical output 0.20724382996559143\n",
      "classical output 0.2100486308336258\n",
      "classical output 0.20371071994304657\n",
      "classical output 0.2068333625793457\n",
      "classical output 0.20554573833942413\n",
      "classical output 0.2120373696088791\n",
      "classical output 0.20672056078910828\n",
      "classical output 0.20241661369800568\n",
      "classical output 0.20326046645641327\n",
      "1.04750394821167\n",
      "ep 67\n",
      "---------------------\n",
      "classical output 0.20491380989551544\n",
      "classical output 0.1986822783946991\n",
      "classical output 0.20441515743732452\n",
      "classical output 0.2118203341960907\n",
      "classical output 0.21472546458244324\n",
      "classical output 0.20819635689258575\n",
      "classical output 0.21144060790538788\n",
      "classical output 0.21000206470489502\n",
      "classical output 0.21657595038414001\n",
      "classical output 0.2111993134021759\n",
      "classical output 0.20675161480903625\n",
      "classical output 0.20763006806373596\n",
      "1.0353550910949707\n",
      "ep 68\n",
      "---------------------\n",
      "classical output 0.2092810869216919\n",
      "classical output 0.2030087262392044\n",
      "classical output 0.20884013175964355\n",
      "classical output 0.2163313329219818\n",
      "classical output 0.21933135390281677\n",
      "classical output 0.212619811296463\n",
      "classical output 0.2159898281097412\n",
      "classical output 0.21440176665782928\n",
      "classical output 0.22104224562644958\n",
      "classical output 0.21559545397758484\n",
      "classical output 0.21100082993507385\n",
      "classical output 0.2119370847940445\n",
      "1.0237069129943848\n",
      "ep 69\n",
      "---------------------\n",
      "classical output 0.2135782539844513\n",
      "classical output 0.2072627991437912\n",
      "classical output 0.2131921797990799\n",
      "classical output 0.22076430916786194\n",
      "classical output 0.22386136651039124\n",
      "classical output 0.21696218848228455\n",
      "classical output 0.2204573154449463\n",
      "classical output 0.2187340259552002\n",
      "classical output 0.2254275530576706\n",
      "classical output 0.21991868317127228\n",
      "classical output 0.21520037949085236\n",
      "classical output 0.21618099510669708\n",
      "1.0125203132629395\n",
      "ep 70\n",
      "---------------------\n",
      "classical output 0.21777883172035217\n",
      "classical output 0.21142339706420898\n",
      "classical output 0.21744635701179504\n",
      "classical output 0.22509042918682098\n",
      "classical output 0.22829532623291016\n",
      "classical output 0.22120916843414307\n",
      "classical output 0.22482042014598846\n",
      "classical output 0.22295689582824707\n",
      "classical output 0.22971686720848083\n",
      "classical output 0.22413599491119385\n",
      "classical output 0.21929028630256653\n",
      "classical output 0.2203286737203598\n",
      "1.0017764568328857\n",
      "ep 71\n",
      "---------------------\n",
      "classical output 0.22191068530082703\n",
      "classical output 0.21551017463207245\n",
      "classical output 0.2216322422027588\n",
      "classical output 0.229349285364151\n",
      "classical output 0.23265807330608368\n",
      "classical output 0.2253815233707428\n",
      "classical output 0.22913379967212677\n",
      "classical output 0.22713808715343475\n",
      "classical output 0.23394271731376648\n",
      "classical output 0.22829756140708923\n",
      "classical output 0.2233159989118576\n",
      "classical output 0.22439409792423248\n",
      "0.9914698600769043\n",
      "ep 72\n",
      "---------------------\n",
      "classical output 0.225957989692688\n",
      "classical output 0.21952266991138458\n",
      "classical output 0.22571489214897156\n",
      "classical output 0.23349428176879883\n",
      "classical output 0.236887127161026\n",
      "classical output 0.22942981123924255\n",
      "classical output 0.233317568898201\n",
      "classical output 0.2311898171901703\n",
      "classical output 0.23805180191993713\n",
      "classical output 0.2323492467403412\n",
      "classical output 0.22723564505577087\n",
      "classical output 0.2283698171377182\n",
      "0.9815663695335388\n",
      "ep 73\n",
      "---------------------\n",
      "classical output 0.22992165386676788\n",
      "classical output 0.2234814465045929\n",
      "classical output 0.22975651919841766\n",
      "classical output 0.237590029835701\n",
      "classical output 0.24108672142028809\n",
      "classical output 0.23344449698925018\n",
      "classical output 0.23744149506092072\n",
      "classical output 0.23518387973308563\n",
      "classical output 0.24206995964050293\n",
      "classical output 0.23632094264030457\n",
      "classical output 0.2310706377029419\n",
      "classical output 0.2322559803724289\n",
      "0.9720578193664551\n",
      "ep 74\n",
      "---------------------\n",
      "classical output 0.233785480260849\n",
      "classical output 0.22733072936534882\n",
      "classical output 0.23370328545570374\n",
      "classical output 0.24160662293434143\n",
      "classical output 0.2452128827571869\n",
      "classical output 0.23739270865917206\n",
      "classical output 0.24150004982948303\n",
      "classical output 0.23911771178245544\n",
      "classical output 0.24605198204517365\n",
      "classical output 0.2402571737766266\n",
      "classical output 0.23486857116222382\n",
      "classical output 0.23610998690128326\n",
      "0.9629228115081787\n",
      "ep 75\n",
      "---------------------\n",
      "classical output 0.23763442039489746\n",
      "classical output 0.23116210103034973\n",
      "classical output 0.23760676383972168\n",
      "classical output 0.2455410212278366\n",
      "classical output 0.24922339618206024\n",
      "classical output 0.24122467637062073\n",
      "classical output 0.24545001983642578\n",
      "classical output 0.24294790625572205\n",
      "classical output 0.24992427229881287\n",
      "classical output 0.2440662682056427\n",
      "classical output 0.23855865001678467\n",
      "classical output 0.2398628294467926\n",
      "0.954149603843689\n",
      "ep 76\n",
      "---------------------\n",
      "classical output 0.24136480689048767\n",
      "classical output 0.23485369980335236\n",
      "classical output 0.2413913458585739\n",
      "classical output 0.24937550723552704\n",
      "classical output 0.2531348764896393\n",
      "classical output 0.24498073756694794\n",
      "classical output 0.24931350350379944\n",
      "classical output 0.24668002128601074\n",
      "classical output 0.2536896765232086\n",
      "classical output 0.24780964851379395\n",
      "classical output 0.24218058586120605\n",
      "classical output 0.24354133009910583\n",
      "0.945707380771637\n",
      "ep 77\n",
      "---------------------\n",
      "classical output 0.24502138793468475\n",
      "classical output 0.23849935829639435\n",
      "classical output 0.24511504173278809\n",
      "classical output 0.2531282603740692\n",
      "classical output 0.2570021152496338\n",
      "classical output 0.24869754910469055\n",
      "classical output 0.2531374990940094\n",
      "classical output 0.2503789961338043\n",
      "classical output 0.2574228346347809\n",
      "classical output 0.25150009989738464\n",
      "classical output 0.24577417969703674\n",
      "classical output 0.24717546999454498\n",
      "0.9375916123390198\n",
      "ep 78\n",
      "---------------------\n",
      "classical output 0.24866202473640442\n",
      "classical output 0.2421155869960785\n",
      "classical output 0.2487833946943283\n",
      "classical output 0.2568410038948059\n",
      "classical output 0.26080387830734253\n",
      "classical output 0.25235918164253235\n",
      "classical output 0.2569035291671753\n",
      "classical output 0.25402912497520447\n",
      "classical output 0.2611176371574402\n",
      "classical output 0.25517502427101135\n",
      "classical output 0.2493457794189453\n",
      "classical output 0.2507990598678589\n",
      "0.9297701716423035\n",
      "ep 79\n",
      "---------------------\n",
      "classical output 0.25227683782577515\n",
      "classical output 0.24571552872657776\n",
      "classical output 0.2524760663509369\n",
      "classical output 0.26057106256484985\n",
      "classical output 0.264617383480072\n",
      "classical output 0.2560064196586609\n",
      "classical output 0.26065462827682495\n",
      "classical output 0.25766900181770325\n",
      "classical output 0.2648039758205414\n",
      "classical output 0.25882595777511597\n",
      "classical output 0.25289154052734375\n",
      "classical output 0.25440070033073425\n",
      "0.9222352504730225\n",
      "ep 80\n",
      "---------------------\n",
      "classical output 0.25588753819465637\n",
      "classical output 0.24932284653186798\n",
      "classical output 0.2561531662940979\n",
      "classical output 0.26429831981658936\n",
      "classical output 0.2684405744075775\n",
      "classical output 0.25968629121780396\n",
      "classical output 0.26441508531570435\n",
      "classical output 0.26129579544067383\n",
      "classical output 0.26848623156547546\n",
      "classical output 0.26247674226760864\n",
      "classical output 0.2564461827278137\n",
      "classical output 0.2579752206802368\n",
      "0.9149917960166931\n",
      "ep 81\n",
      "---------------------\n",
      "classical output 0.2594466805458069\n",
      "classical output 0.25287294387817383\n",
      "classical output 0.25977855920791626\n",
      "classical output 0.2679438292980194\n",
      "classical output 0.27215319871902466\n",
      "classical output 0.2632583975791931\n",
      "classical output 0.26808804273605347\n",
      "classical output 0.26485103368759155\n",
      "classical output 0.2720659375190735\n",
      "classical output 0.2660296559333801\n",
      "classical output 0.2599171996116638\n",
      "classical output 0.26148849725723267\n",
      "0.9080151319503784\n",
      "ep 82\n",
      "---------------------\n",
      "classical output 0.26295822858810425\n",
      "classical output 0.2563773989677429\n",
      "classical output 0.263343870639801\n",
      "classical output 0.2715457081794739\n",
      "classical output 0.27579814195632935\n",
      "classical output 0.26678791642189026\n",
      "classical output 0.2717127203941345\n",
      "classical output 0.26834264397621155\n",
      "classical output 0.2755846381187439\n",
      "classical output 0.26952093839645386\n",
      "classical output 0.2633271813392639\n",
      "classical output 0.2649041414260864\n",
      "0.9013062119483948\n",
      "ep 83\n",
      "---------------------\n",
      "classical output 0.2663772404193878\n",
      "classical output 0.2597794532775879\n",
      "classical output 0.2667883634567261\n",
      "classical output 0.2750247120857239\n",
      "classical output 0.27935314178466797\n",
      "classical output 0.27021437883377075\n",
      "classical output 0.27522724866867065\n",
      "classical output 0.2717280089855194\n",
      "classical output 0.2789829671382904\n",
      "classical output 0.27290546894073486\n",
      "classical output 0.2666151523590088\n",
      "classical output 0.2682226896286011\n",
      "0.8948472738265991\n",
      "ep 84\n",
      "---------------------\n",
      "classical output 0.2696932554244995\n",
      "classical output 0.2631050944328308\n",
      "classical output 0.27015504240989685\n",
      "classical output 0.2784097194671631\n",
      "classical output 0.2828231751918793\n",
      "classical output 0.27357572317123413\n",
      "classical output 0.27864640951156616\n",
      "classical output 0.27503398060798645\n",
      "classical output 0.282284677028656\n",
      "classical output 0.27617090940475464\n",
      "classical output 0.2698121666908264\n",
      "classical output 0.27144259214401245\n",
      "0.8886167407035828\n",
      "ep 85\n",
      "---------------------\n",
      "classical output 0.2729248106479645\n",
      "classical output 0.2663411498069763\n",
      "classical output 0.27344247698783875\n",
      "classical output 0.2816856801509857\n",
      "classical output 0.28615960478782654\n",
      "classical output 0.27680957317352295\n",
      "classical output 0.2819752097129822\n",
      "classical output 0.2782597541809082\n",
      "classical output 0.2855165898799896\n",
      "classical output 0.2793729901313782\n",
      "classical output 0.27292460203170776\n",
      "classical output 0.2745662331581116\n",
      "0.882605254650116\n",
      "ep 86\n",
      "---------------------\n",
      "classical output 0.276043176651001\n",
      "classical output 0.26944905519485474\n",
      "classical output 0.2765992283821106\n",
      "classical output 0.2848636507987976\n",
      "classical output 0.2894168496131897\n",
      "classical output 0.2799660563468933\n",
      "classical output 0.28520649671554565\n",
      "classical output 0.2813568711280823\n",
      "classical output 0.2886292338371277\n",
      "classical output 0.28246572613716125\n",
      "classical output 0.27594900131225586\n",
      "classical output 0.2776026129722595\n",
      "0.8767877817153931\n",
      "ep 87\n",
      "---------------------\n",
      "classical output 0.27904433012008667\n",
      "classical output 0.27244389057159424\n",
      "classical output 0.27964362502098083\n",
      "classical output 0.2878897786140442\n",
      "classical output 0.29250648617744446\n",
      "classical output 0.28294166922569275\n",
      "classical output 0.2882689833641052\n",
      "classical output 0.2843164801597595\n",
      "classical output 0.2915930449962616\n",
      "classical output 0.2854154109954834\n",
      "classical output 0.27881163358688354\n",
      "classical output 0.2804652154445648\n",
      "0.8711968064308167\n",
      "ep 88\n",
      "---------------------\n",
      "classical output 0.2819007635116577\n",
      "classical output 0.27528488636016846\n",
      "classical output 0.2825165390968323\n",
      "classical output 0.29076939821243286\n",
      "classical output 0.2954563498497009\n",
      "classical output 0.2857735753059387\n",
      "classical output 0.29119572043418884\n",
      "classical output 0.28712448477745056\n",
      "classical output 0.29440242052078247\n",
      "classical output 0.288226842880249\n",
      "classical output 0.28156542778015137\n",
      "classical output 0.28323623538017273\n",
      "0.8658294677734375\n",
      "ep 89\n",
      "---------------------\n",
      "classical output 0.2846779227256775\n",
      "classical output 0.27808505296707153\n",
      "classical output 0.28536733984947205\n",
      "classical output 0.2936151623725891\n",
      "classical output 0.2983761429786682\n",
      "classical output 0.28858813643455505\n",
      "classical output 0.2941031754016876\n",
      "classical output 0.2899417579174042\n",
      "classical output 0.2972103953361511\n",
      "classical output 0.29100966453552246\n",
      "classical output 0.2842736542224884\n",
      "classical output 0.2859629690647125\n",
      "0.86065673828125\n",
      "ep 90\n",
      "---------------------\n",
      "classical output 0.287402480840683\n",
      "classical output 0.2808310389518738\n",
      "classical output 0.28816595673561096\n",
      "classical output 0.2964073121547699\n",
      "classical output 0.3012259900569916\n",
      "classical output 0.2913433015346527\n",
      "classical output 0.29692474007606506\n",
      "classical output 0.29264122247695923\n",
      "classical output 0.29991474747657776\n",
      "classical output 0.2937048077583313\n",
      "classical output 0.2869139313697815\n",
      "classical output 0.28861841559410095\n",
      "0.8556539416313171\n",
      "ep 91\n",
      "---------------------\n",
      "classical output 0.29003944993019104\n",
      "classical output 0.2834464907646179\n",
      "classical output 0.29081615805625916\n",
      "classical output 0.29905110597610474\n",
      "classical output 0.3039418160915375\n",
      "classical output 0.29394984245300293\n",
      "classical output 0.29961055517196655\n",
      "classical output 0.2952319085597992\n",
      "classical output 0.30250343680381775\n",
      "classical output 0.2962912321090698\n",
      "classical output 0.28945690393447876\n",
      "classical output 0.291183739900589\n",
      "0.8508113622665405\n",
      "ep 92\n",
      "---------------------\n",
      "classical output 0.2926101088523865\n",
      "classical output 0.28602010011672974\n",
      "classical output 0.2934393584728241\n",
      "classical output 0.3016834259033203\n",
      "classical output 0.3066403567790985\n",
      "classical output 0.2965802848339081\n",
      "classical output 0.30228596925735474\n",
      "classical output 0.29782819747924805\n",
      "classical output 0.30510181188583374\n",
      "classical output 0.2988905906677246\n",
      "classical output 0.2920154929161072\n",
      "classical output 0.29375889897346497\n",
      "0.8461295962333679\n",
      "ep 93\n",
      "---------------------\n",
      "classical output 0.2951948642730713\n",
      "classical output 0.2886022925376892\n",
      "classical output 0.2960500717163086\n",
      "classical output 0.3042905926704407\n",
      "classical output 0.30931681394577026\n",
      "classical output 0.2991999685764313\n",
      "classical output 0.3049456477165222\n",
      "classical output 0.30037838220596313\n",
      "classical output 0.30764269828796387\n",
      "classical output 0.30142033100128174\n",
      "classical output 0.29450371861457825\n",
      "classical output 0.29628244042396545\n",
      "0.8415899872779846\n",
      "ep 94\n",
      "---------------------\n",
      "classical output 0.2977201044559479\n",
      "classical output 0.2911401689052582\n",
      "classical output 0.29863911867141724\n",
      "classical output 0.306869238615036\n",
      "classical output 0.3119523525238037\n",
      "classical output 0.3017660975456238\n",
      "classical output 0.3075810372829437\n",
      "classical output 0.30291080474853516\n",
      "classical output 0.31015872955322266\n",
      "classical output 0.30392810702323914\n",
      "classical output 0.2969669699668884\n",
      "classical output 0.2987556457519531\n",
      "0.8371856808662415\n",
      "ep 95\n",
      "---------------------\n",
      "classical output 0.3002067506313324\n",
      "classical output 0.29362279176712036\n",
      "classical output 0.3011623024940491\n",
      "classical output 0.3093951344490051\n",
      "classical output 0.314546674489975\n",
      "classical output 0.30429333448410034\n",
      "classical output 0.31018495559692383\n",
      "classical output 0.3054250180721283\n",
      "classical output 0.31266459822654724\n",
      "classical output 0.3064402937889099\n",
      "classical output 0.29940730333328247\n",
      "classical output 0.301196813583374\n",
      "0.8329189419746399\n",
      "ep 96\n",
      "---------------------\n",
      "classical output 0.30266281962394714\n",
      "classical output 0.29606759548187256\n",
      "classical output 0.3036354184150696\n",
      "classical output 0.31186750531196594\n",
      "classical output 0.31707945466041565\n",
      "classical output 0.3067683279514313\n",
      "classical output 0.31272727251052856\n",
      "classical output 0.30783987045288086\n",
      "classical output 0.31505125761032104\n",
      "classical output 0.3088090717792511\n",
      "classical output 0.3017314076423645\n",
      "classical output 0.303545743227005\n",
      "0.8287709951400757\n",
      "ep 97\n",
      "---------------------\n",
      "classical output 0.30504947900772095\n",
      "classical output 0.29844993352890015\n",
      "classical output 0.3060305714607239\n",
      "classical output 0.3142549395561218\n",
      "classical output 0.31952327489852905\n",
      "classical output 0.3091305196285248\n",
      "classical output 0.3151732385158539\n",
      "classical output 0.31019848585128784\n",
      "classical output 0.3173721134662628\n",
      "classical output 0.3111400008201599\n",
      "classical output 0.30401962995529175\n",
      "classical output 0.30585697293281555\n",
      "0.8247568607330322\n",
      "ep 98\n",
      "---------------------\n",
      "classical output 0.3073801100254059\n",
      "classical output 0.3007887303829193\n",
      "classical output 0.3084002137184143\n",
      "classical output 0.3166118264198303\n",
      "classical output 0.32196810841560364\n",
      "classical output 0.31151220202445984\n",
      "classical output 0.3176288902759552\n",
      "classical output 0.3125576376914978\n",
      "classical output 0.31973737478256226\n",
      "classical output 0.3134945034980774\n",
      "classical output 0.30630287528038025\n",
      "classical output 0.3081605136394501\n",
      "0.8208518028259277\n",
      "ep 99\n",
      "---------------------\n",
      "classical output 0.3096911311149597\n",
      "classical output 0.3030906319618225\n",
      "classical output 0.3107507824897766\n",
      "classical output 0.3189486265182495\n",
      "classical output 0.32436078786849976\n",
      "classical output 0.3138267397880554\n",
      "classical output 0.3199911415576935\n",
      "classical output 0.3148525655269623\n",
      "classical output 0.32204288244247437\n",
      "classical output 0.31579914689064026\n",
      "classical output 0.3085443377494812\n",
      "classical output 0.3104301393032074\n",
      "0.8170480132102966\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "NN = FullyConnectedNetwork(alpha=0.01, reg_type='l2', lambda_=0.0007)\n",
    "\n",
    "NN.add_layer((784, 784), 'ReLU')\n",
    "NN.add_layer((784, 256), 'ReLU')\n",
    "NN.add_layer((256, 128), 'ReLU')\n",
    "NN.add_layer((128, 10), 'Softmax', class_number=10)\n",
    "\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][0].__dict__['_FullyConnectedLayer__weights'] = init_weight1.T\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][0].__dict__['_FullyConnectedLayer__bias'] = init_bias1\n",
    "\n",
    "\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][1].__dict__['_FullyConnectedLayer__weights'] = init_weight2.T\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][1].__dict__['_FullyConnectedLayer__bias'] = init_bias2\n",
    "\n",
    "\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][2].__dict__['_FullyConnectedLayer__weights'] = init_weight3.T\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][2].__dict__['_FullyConnectedLayer__bias'] = init_bias3\n",
    "\n",
    "\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][3].__dict__['_FullyConnectedLayer__weights'] = init_weight4.T\n",
    "NN.__dict__['_FullyConnectedNetwork__layers'][3].__dict__['_FullyConnectedLayer__bias'] = init_bias4\n",
    "\n",
    "loss_tr, loss_val, acc_tr, acc_val = train(NN, 100, use_old_weight=True, decay_sched={1000: 0.002})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7943ac595c30>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_tr, label='my')\n",
    "plt.plot(l, label='torch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7943bf649960>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_tr, label='my')\n",
    "plt.plot(l, label='torch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7943bedc4820>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_tr, label='my')\n",
    "plt.plot(l, label='torch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7943ac487880>]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.abs(np.array(l) - np.array(list(map(lambda x: x.item(), loss_tr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7943c0833700>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(l) - np.array(list(map(lambda x: x.item(), loss_tr))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "multilayer_fully_connected_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
